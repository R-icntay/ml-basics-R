---
title: 'Explore and analyze data with R'
output:
  html_document:
    #css: style_7.css
    df_print: paged
    theme: flatly
    highlight: breezedark
    toc: yes
    toc_float: yes
    code_download: yes
---

## **Exploring Data with R**

A significant part of a a data scientist's role is to explore, analyze, and visualize data. There's a wide range of tools and programming languages that they can use to do this; and of the most popular approaches is to use Jupyter notebooks (like this one) and the R language.

R is a flexible, intuitive and elegant programming language that is used in a wide range of industries; from banking, insurance, pharmaceutical, genetics, telecommunications, marketing all the way to healthcare. Some reasons why we think R is a great place to start your data science adventure are:

-   it's natively designed to support data science.

-   R's diverse and welcoming community. Just to name a few: [R-Ladies](http://r-ladies.org/), [Minority in R](https://medium.com/@doritolay/introducing-mir-a-community-for-underrepresented-users-of-r-7560def7d861) and [the \#rstats twitter community](https://twitter.com/search?q=%23rstats).

-   R has a massive set of packages for wrangling data, statistical modelling and machine learning. So in case you get stuck on a problem, there is a high chance that someone has tried to do it and you can learn or build up on their work.

In this notebook, we'll explore some of these packages, and apply basic techniques to analyze data. This is not intended to be a comprehensive R programming exercise; or even a deep dive into data analysis. Rather, it's intended as a crash course in some of the common ways in which data scientists can use R to work with data.

> **Note**: If you've never used the Jupyter Notebooks environment before, there are a few things you should be aware of:
>
> -   Notebooks are made up of *cells*. Some cells (like this one) contain *markdown* text, while others (like the one beneath this one) contain code.
> -   You can run each code cell by using the **â–º Run** button. the **â–º Run** button will show up when you hover over the cell.
> -   The output from each code cell will be displayed immediately below the cell.
> -   Even though the code cells can be run individually, some variables used in the code are global to the notebook. That means that you should run all of the code cells **in order**. There may be dependencies between code cells, so if you skip a cell, subsequent cells might not run correctly.

### **Tibbles**

Tibbles (data frames) are one of the biggest and most important ideas in R and still, one of the most common and useful storage structures for data analysis in R. As such, we think it's a good idea to start with learning/working with tibbles since it immediately pays off in both data transformation and visualization. We'll work through other data structure in R as we progress along the course.

Tibbles are provided by the **tibble** package, which is part of the core tidyverse. So, let's take a trip into the tidyverse!

```{r}
# Load the packages in the tidyverse into the current R session
library(tidyverse)
```

From the startup message, we can see that we have loaded/attached a bunch of packages with `tibble` included. It also shows **Conflicts**, which are functions from the tidyverse which conflict with other functions in base R or other packages you might have loaded. Don't worry about these for now.

Good job! That means, we can now create our first tibble of students' data with `tibble()` as shown below:

```{r}
# Build a tibble of student data
df_students <- tibble(
  
  # Student names
  name = c('Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky',
           'Frederic', 'Jimmie', 'Rhonda', 'Giovanni',
           'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny',
           'Jakeem','Helena','Ismat','Anila','Skye','Daniel',
           'Aisha'),
  
  # Study hours
  study_hours = c(10.0, 11.5, 9.0, 16.0, 9.25, 1.0, 11.5, 9.0,
                 8.5, 14.5, 15.5, 13.75, 9.0, 8.0, 15.5, 8.0,
                 9.0, 6.0, 10.0, 12.0, 12.5, 12.0),
  
  # Grades
  grade = c(50, 50, 47, 97, 49, 3, 53, 42, 26,
             74, 82, 62, 37, 15, 70, 27, 36, 35,
             48, 52, 63, 64)
)

# Print the tibble
df_students
```

Yes! There goes our first tibble! You may have noticed that we created the tibble from individual elements: `name`, `study_hours` and `grade`. These are called `vectors` and are created using `c()`, short for `combine`. The length of each vector in a tibble must be the same - a property that gives tibbles their rectangular structure.

You might also have noticed some abbreviations under the column names. These abbreviations describe the type of each column

-   `chr` stands for character vectors, or strings.

-   `dbl` stands for doubles, or real numbers that may have a floating point standard.

We'll encounter more as we make progress in this adventure!

### Loading a data frame from a file

We constructed a data frame from some existing vectors which we typed by hand. However, this could invite typos and errors. In many real-world scenarios, data is loaded from sources such as files. You can then ask R to read the file and store the contents as an object.

Let's replace the student grades data frame with the contents of a CSV file.

```{r}
# Read a CSV file into a tibble
students <- read_csv(file = "https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/grades.csv")

# Print the first 10 rows of the data
slice_head(students, n = 10)
```

`read_csv()` is used to import a csv file into a tibble. `slice_head()` allows you to return the first `n` rows of a tibble.

slice_head() is actually a variant of the function `slice`. slice() allows you to select rows based on their integer location. Let's say you wanted to narrow down to rows at integer positions 5 to 10, here's one way you could approach that:

```{r}
slice(students, n = 5:10)
```

> Indexing in R starts at `1`.

### **Explore tibbles with dplyr.**

Now that we have some data, we can begin to solve some of the common data manipulation challenges:

#### **Filter rows with dplyr::filter()**

The `filter()` function is used to create a subset of the original data which contains rows that satisfy your conditions.

Let's say we are particularly interested in `Jenny's` performance. As such, we **filter** the **students** data to only keep rows where the entry for **Name** exactly matches "**Jenny**".

```{r}
filter(students, Name == "Jenny")
```

Perhaps we are also interested in `Giovanni`'s performance. So this means we want to keep rows where the name matches Jenny **OR** Giovanni. We can approach the problem using the `%in%` operator, followed by a vector of values to look for a match in:

```{r}
filter(students, Name %in% c("Jenny", "Giovanni"))
```

Here, we just asked filter() to select all rows where the observations in the Name column match those provided in the vector of values.

Sometimes, we want to retain rows whose conditions are met in multiple columns. Say we wanted to find the students who studied for more than 12 hours **and** got a grade of more than 80. In this case, a row should only be retained if both of those conditions are met. An **and** statement within filter() can be expressed as follows:

-   using a comma between conditions

```{r}
filter(students, StudyHours > 12, Grade > 80)
```

-   using an ampersand between conditions

```{r}
filter(students, StudyHours > 12 & Grade > 80)
```

#### **Introducing the pipe operator (%\>%)**

The pipe operator (`%>%`) is used to perform operations in logical sequence by passing an object forward into a function or call expression. You can think of the pipe operator as saying "and then" in your code.

The pipe operator is one of the functions in R that will really help you to work intuitively with the data - enabling you to translate what you have in mind to actual code.

For instance, lets say we want to filter for the student Bill. We can think of it like this:

we are telling R to take the **students** data frame, **AND THEN** **`filter`** the rows that contain the **Name** **Bill**. This can be translated into code like this:

```{r}
# Take the students data frame AND THEN filter for the Name "Bill"
students %>% 
  filter(Name == "Bill")
```

Such a superpower, right?

Look at what we have for Bill's grade: `NA`. This brings us to the next adventure.

#### Handling missing values

One of the most common issues data scientists need to deal with is incomplete or missing data. R represents missing, or unknown values, with special sentinel value: `NA` (Not Available).

So how would we know that the data frame contains missing values?

-   One way to quickly investigate whether our data contains any `NA`s the function `anyNA` which returns `TRUE` or `FALSE`

```{r}
anyNA(students)
```

This tells us that indeed, there are some missing values.

-   Another way would be to use the function `is.na()` which indicates which individual elements are missing with a logical `TRUE`.

```{r}
is.na(students)
```

Okay, got the job done but with a larger data frame, it would be inefficient and practically impossible to review all of the rows and columns individually ðŸ˜´.

-   Another more intuitive way would be to get the sum of missing values for each column, like this:

```{r}
colSums(is.na(students))
```

So now we know that there's `one` missing value in the **StudyHours** column, and `two` missing values in the **Grade** column.

We can also get the sum of missing values for each row, which could be more useful since `filter` is primarily used to subset rows.

```{r}
rowSums(is.na(students))
```

This tells us that the last row has two `NA`s while the second last row has one missing value. Great! This means that we can tell R to **filter** the rows where the sum of NAs is greater than 0:

```{r}
students %>% 
  filter(rowSums(is.na(students)) > 0)
```

Now that we've found the missing values, what can we do about them?

One common approach is to *impute* replacement values. For example, if the number of study hours is missing, we could just assume that the student studied for an average amount of time and replace the missing value with the mean study hours. This begs the question, how do we modify existing columns?

#### **Create and modify columns using dplyr::mutate()**

`mutate()` is used to add or modify columns and preserves existing ones. Here is how we would go about in replacing the missing values found in the **StudyHours** column, with the mean study hours:

```{r}
# Replace NA in column StudyHours with the mean study hours
students <- students %>% 
  mutate(StudyHours = replace_na(StudyHours, mean(StudyHours, na.rm = TRUE)))

# Print the data frame
students
```

> `na.rm = TRUE` argument is added to exclude missing values

Awesome! You have just replaced the missing value in the StudyHour column, with the mean study hours. In the process, you have also learnt a new function: `replace_na()`. `tidyr::replace_na` replaces missing values with specified values.

> [*`tidyr`*](https://tidyr.tidyverse.org/index.html) *is a part of the tidyverse and its goal is to help you tidy messy data.*

Alternatively, it might be important to ensure that you only use data you know to be absolutely correct. So let's drop rows that contain missing values by using `tidyr::drop_na` function.

```{r}
# Drop NAs from our tibble
students <- students %>% 
  drop_na()

# Print tibble
students
```

Just to be sure ðŸ˜…:

```{r}
anyNA(students)
```

Now that we've cleaned up the missing values, we're ready to do more meaningful data exploration. Let's start by comparing the mean study hours and grades. This requires us to extract the numeric values of the individual column and pass them to the `mean()` function. `$` and `dplyr::pull()` do exactly this.

```{r}
# Get the mean study hours using the accessor `$`
mean_study <- mean(students$StudyHours)

# Get the mean grade using dplyr::pull
mean_grade <- students %>% 
  pull(Grade) %>% 
  mean()

# Print the mean study hours and mean grade
cat(
  'Average weekly study hours: ', round(mean_study, 2),
   '\nAverage grade: ', round(mean_grade, 2)
)
```

With this information, we may want to filter the data frame to find only the students who studied for more than the average amount of time.

```{r}
# Get students who studied for more than average hours
students %>% 
  filter(StudyHours > mean_study)
```

Note that the filtered result retained the attributes of the original tibble and is itself a tibble, so you can work with its rows and columns just like any other tibble.

For example, let's find the average grade for students who undertook more than the average amount of study time.

```{r}
# Mean grade of students who studied more than average hours
students %>% 
  filter(StudyHours > mean_study) %>% 
  pull(Grade) %>% 
  mean()
```

Fantastic! Let's assume that the passing grade for the course is 60.

We can use that information to add a new column to the data frame, indicating whether or not each student passed(TRUE/FALSE). Again, this calls for `dplyr::mutate()`. We have previously seen how to use mutate to modify existing columns. We'll now use mutate to add a new column.

The general structure of adding new columns is basically the same as before:

`df %>% mutate(new_column_name = what_it_contains)`

Let's go forth and mutate!

```{r}
# TRUE/FALSE column based on whether student passed or not
students <- students %>% 
                  mutate(Pass = Grade >= 60)

# Print data frame
students
```

Awesome! We have added a new column **Pass** of type `lgl`

-   `lgl` stands for logical, vectors that contain only `TRUE` or `FALSE`.

#### **Grouped summaries**

Data frames are designed for rectangular data, and you can use them to perform many of the kinds of data analytics operation you can do in a relational database; such as grouping and aggregating tables of data. In R, this is achieved using `dplyr::group_by() %>% summarize()`

-   `dplyr::group_by()` changes the unit of analysis from the complete dataset to individual groups.

-   `dplyr::summarize()` creates a new data frame for summary statistics that you have specified.

For example, we can use the `dplyr::group_by() %>% summarize()` to group the student data into groups based on the **Pass** column and then find the mean study time and grade for the groups of students who passed and failed the course.

```{r}
# Mean study time and grade for students who passed or failed the course
students %>% 
  group_by(Pass) %>% 
  summarise(mean_study = mean(StudyHours), mean_grade = mean(Grade))
```

Let's say we had many numeric columns and still wanted to apply the function, `mean`, across them.

`dplyr::across` makes it easy to apply a function across multiple columns. To demonstrate, let's apply `across` to our previous example.

```{r}
# Mean study time and Grade for students who passed or failed the course
students %>% 
  group_by(Pass) %>% 
  summarise(across(where(is.numeric), mean))
  
```

What if we wanted to determine how many students passed or failed? We'd have to group our data based on the `Pass` column, then do a tally for each group. `dplyr::count()` wraps all this to give you a nice grouped count like this:

```{r}
# Grouped count for Pass column
students %>% 
  count(Pass)
```

Pretty succinct, right?

#### **Select columns with dplyr::select()**

At times, we my be faced with datasets with hundreds of columns and may want to narrow down to some columns of interest. `select()` allows you to pick or exclude columns in a data frame.

Say we wanted to only pick the Name and StudyHours column, here's how we would approach the problem:

```{r}
# Select the Name and StudyHours  column
students %>% 
  select(Name, StudyHours)
```

In some scenarios, it may be more convenient to drop a specific column in contrast to selecting all the other columns. This is achieved using the `!` operator.

For instance, let's keep all other columns except the StudyHours column:

```{r}
# Keep all columns except the StudyHours column
select(students, !StudyHours)
```

Good job!

`select` also has a number of helpers that allows you to select variables based on their properties. For instance, we may only be interested in columns **where** the observations are **numeric**. Here is how we would achieve that:

```{r}
# Select numeric columns
students %>% 
  select(where(is.numeric))
```

`select()` can make working with data sets with many variables more manageable.

#### **Order rows with dplyr::arrange()**

Let's wrap this adventure by sorting the student data into descending order of Grade, and assigning the resulting sorted data frame to the variable name **students_sorted**.

To do this, we'll need to reach into dplyr and take one more verb: `arrange()`. It orders the rows of a data frame by column values.

```{r}
# Create a data frame with the data sorted by Grade (descending)
students_sorted <- students %>%
  # Sort by descending order
  arrange(desc(Grade))

# Print data frame
students_sorted
```

> To get help on any function, let's say `arrange()`, run the following command
>
> `?arrange`

## **Summary**

That's it for now!

It is often rare that you will get the data in exactly the right form you need.

Luckily, `dplyr` provides simple `verbs`, that is, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.

With the six verbs that we learnt (`filter`, `arrange`, `select`, `mutate`, `group_by` and `summarise`), we are well on our way to to solving the vast majority of the data manipulation challenges.

In our next workbook, we'll take a look at how create graphs and explore your data in more interesting ways.

## **Further Reading**

To learn more about the R packages you explored in this notebook, see the following documentation:

-   [Tidyverse packages](https://www.tidyverse.org/packages/)

```{r}
library(here)
library(rmd2jupyter)
rmd2jupyter("01_Data_Exploration_Exercise_01.Rmd")
```
