{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## **Exercise - Train and evaluate multiclass classification models**\n",
                "\n",
                "Binary classification techniques work well when the data observations belong to one of two classes or categories, such as `True` or `False`. When the data can be categorized into `more than two classes`, you must use a `multiclass classification algorithm`.\n",
                "\n",
                "Multiclass classification can be thought of as a `combination` of `multiple binary classifiers`. There are two ways in which you approach the problem:\n",
                "\n",
                "-   **One vs Rest (OVR)**, in which a classifier is created for each possible class value, with a positive outcome for cases where the prediction is *this* class, and negative predictions for cases where the prediction is any other class. A classification problem with four possible shape classes (*square*, *circle*, *triangle*, *hexagon*) would require four classifiers that predict:\n",
                "\n",
                "    -   *square* or not\n",
                "\n",
                "    -   *circle* or not\n",
                "\n",
                "    -   *triangle* or not\n",
                "\n",
                "    -   *hexagon* or not\n",
                "\n",
                "-   **One vs One (OVO)**, in which a classifier for each possible pair of classes is created. The classification problem with four shape classes would require the following binary classifiers:\n",
                "\n",
                "    -   *square* or *circle*\n",
                "\n",
                "    -   *square* or *triangle*\n",
                "\n",
                "    -   *square* or *hexagon*\n",
                "\n",
                "    -   *circle* or *triangle*\n",
                "\n",
                "    -   *circle* or *hexagon*\n",
                "\n",
                "    -   *triangle* or *hexagon*\n",
                "\n",
                "In both approaches, the overall model that combines the classifiers generates a vector of predictions in which the probabilities generated from the individual binary classifiers are used to determine which class to predict.\n",
                "\n",
                "Fortunately, in most machine learning frameworks, including Tidymodels, implementing a multiclass classification model is not significantly more complex than binary classification.\n",
                "\n",
                "### Meet the data\n",
                "\n",
                "In this sections, we'll build a multiclass classifier for classifying penguins!\n",
                "\n",
                "The `palmerpenguins` data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n",
                "\n",
                "> These data were collected from 2007 - 2009 by Dr.¬†Kristen Gorman with the [Palmer Station Long Term Ecological Research Program](https://pal.lternet.edu/), part of the [US Long Term Ecological Research Network](https://lternet.edu/). The data were imported directly from the [Environmental Data Initiative](https://environmentaldatainitiative.org/) (EDI) Data Portal, and are available for use by CC0 license (\"No Rights Reserved\") in accordance with the [Palmer Station Data Policy](https://pal.lternet.edu/data/policies).\n",
                "\n",
                "In R, the package `palmerpenguins` by [Allison Marie Horst and Alison Presmanes Hill and Kristen B Gorman](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) provides us with data related to these adorable creatures.\n",
                "\n",
                "The corresponding csv data used in the Python tutorial can be found [here](https://github.com/MicrosoftDocs/ml-basics/tree/master/data).\n",
                "\n",
                "Care for a pun?\n",
                "\n",
                "    What is a penguin‚Äôs favorite movie?\n",
                "\n",
                "    Frozen ‚ùÑÔ∏èüëß.\n",
                "\n",
                "With that, let's get started exploring some penguins üêßüêß!\n",
                "\n",
                "### Explore the data\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load the required packages and make them available in your current R session\r\n",
                "suppressPackageStartupMessages({\r\n",
                "  library(tidyverse)\r\n",
                "  library(palmerpenguins)\r\n",
                "  library(tidymodels)\r\n",
                "})\r\n",
                "\r\n",
                "# Take a peek into the data\r\n",
                "glimpse(penguins)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The data contains the following columns:\n",
                "\n",
                "-   **species:** a factor denoting the penguin species (`Adelie`, `Chinstrap`, or `Gentoo`)\n",
                "\n",
                "-   **island:** a factor denoting the island (in Palmer Archipelago, Antarctica) where observed\n",
                "\n",
                "-   **bill_length_mm (aka culmen_length):** a number denoting length of the dorsal ridge of penguin bill (millimeters)\n",
                "\n",
                "-   **bill_depth_mm (aka culmen_depth):** a number denoting the depth of the penguin bill (millimeters)\n",
                "\n",
                "-   **flipper_length_mm:** an integer denoting penguin flipper length (millimeters)\n",
                "\n",
                "-   **body_mass_g:** an integer denoting penguin body mass (grams)\n",
                "\n",
                "-   **sex:** a factor denoting penguin sex (male, female)\n",
                "\n",
                "-   **year:** an integer denoting the study year (2007, 2008, or 2009)\n",
                "\n",
                "The **species** column containing penguin species `Adelie`, `Chinstrap`, or `Gentoo`, is the label we want to train a model to predict.\n",
                "\n",
                "The corresponding [Python learning module](https://docs.microsoft.com/en-us/learn/modules/train-evaluate-classification-models/) used a data set with the following variables: **bill_length_mm**, **bill_depth_mm**, **flipper_length_mm**, **body_mass_g**, **species**.\n",
                "\n",
                "Let's narrow down to those and make some summary statistics while at it. The [skimr package](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html) provides a strong set of summary statistics that are generated for a variety of different data types.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load the skimr column\r\n",
                "library(skimr)\r\n",
                "\r\n",
                "# Select desired columns\r\n",
                "penguins_select <- penguins %>% \r\n",
                "  select(c(bill_length_mm, bill_depth_mm, flipper_length_mm,\r\n",
                "           body_mass_g, species))\r\n",
                "\r\n",
                "# Do some summary statistics\r\n",
                "penguins_select %>% \r\n",
                "  skim()\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "From the neat summary provided by skimr, we can see that each our predictor columns contains missing 2 values while our label/outcome column contains none.\n",
                "\n",
                "Let's dig a little deeper and filter the rows that contain missing values.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "penguins_select %>% \n",
                "  filter(if_any(everything(), is.na))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "There are two rows that contain no feature values at all (`NA` stands for Not Available ), so these won't be useful in training a model. Let's discard them from the dataset.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Drop rows containing missing values\n",
                "penguins_select <- penguins_select %>% \n",
                "  drop_na()\n",
                "\n",
                "# Confirm there are no missing values\n",
                "penguins_select %>% \n",
                "  anyNA()\n",
                "\n",
                "# Proportion of each species in the data\n",
                "penguins_select %>% \n",
                "  count(species)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now that we've dealt with the missing values, let's explore how the features relate to the label by creating some box charts.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load the paletteer package\n",
                "library(paletteer)\n",
                "\n",
                "# Pivot data to a long format\n",
                "penguins_select_long <- penguins_select %>% \n",
                "  pivot_longer(!species, names_to = \"predictors\", values_to = \"values\")\n",
                "\n",
                "# Make box plots\n",
                "theme_set(theme_light())\n",
                "penguins_select_long %>%\n",
                "  ggplot(mapping = aes(x = species, y = values, fill = predictors)) +\n",
                "  geom_boxplot() +\n",
                "  facet_wrap(~predictors, scales = \"free\") +\n",
                "  scale_fill_paletteer_d(\"nbapalettes::supersonics_holiday\") +\n",
                "  theme(legend.position = \"none\")\n",
                "  \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "From the box plots, it looks like species `Adelie` and `Chinstrap` have similar data profiles for bill_depth, flipper_length, and body_mass, but Chinstraps tend to have longer bill_length. `Gentoo` tends to have fairly clearly differentiated features from the others; which should help us train a good classification model.\n",
                "\n",
                "### Prepare the data\n",
                "\n",
                "Just as for binary classification, before training the model, we need to split the data into subsets for training and validation. We'll also apply a `stratification` technique when splitting the data to `maintain the proportion of each label value` in the training and validation datasets.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# For reproducibility\n",
                "set.seed(2056)\n",
                "\n",
                "# Split data 70%-30% into training set and test set\n",
                "penguins_split <- penguins_select %>% \n",
                "  initial_split(prop = 0.70, strata = species)\n",
                "\n",
                "# Extract data in each split\n",
                "penguins_train <- training(penguins_split)\n",
                "penguins_test <- testing(penguins_split)\n",
                "\n",
                "# Print the number of observations in each split\n",
                "cat(\"Training cases: \", nrow(penguins_train), \"\\n\",\n",
                "    \"Test cases: \", nrow(penguins_test), sep = \"\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Train and evaluate a multiclass classifier\n",
                "\n",
                "Now that we have a set of training features and corresponding training labels, we can fit a multiclass classification algorithm to the data to create a model.\n",
                "\n",
                "`parsnip::multinom_reg()` defines a model that uses linear predictors to predict multiclass data using the multinomial distribution.\n",
                "\n",
                "Let's fit Multinomial regression via [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) package. This model usually has 1 tuning hyperparameter, `penalty`, which describes the amount of regularization. This is used to counteract any bias in the sample, and help the model generalize well by avoiding *overfitting* the model to the training data. We can of course tune this parameter, like we will later on in this lesson, but for now, let's choose an arbitrary value of `1`\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Specify a multinomial regression via nnet\r\n",
                "multireg_spec <- multinom_reg(penalty = 1) %>% \r\n",
                "  set_engine(\"nnet\") %>% \r\n",
                "  set_mode(\"classification\")\r\n",
                "\r\n",
                "# Train a multinomial regression model without any preprocessing\r\n",
                "set.seed(2056)\r\n",
                "multireg_fit <- multireg_spec %>% \r\n",
                "  fit(species ~ ., data = penguins_train)\r\n",
                "\r\n",
                "# Print the model\r\n",
                "multireg_fit\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now we can use the trained model to predict the labels for the test features, and evaluate performance:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Make predictions for the test set\r\n",
                "penguins_results <- penguins_test %>% select(species) %>% \r\n",
                "  bind_cols(multireg_fit %>% \r\n",
                "              predict(new_data = penguins_test)) %>% \r\n",
                "  bind_cols(multireg_fit %>% \r\n",
                "              predict(new_data = penguins_test, type = \"prob\"))\r\n",
                "\r\n",
                "# Print predictions\r\n",
                "penguins_results %>% \r\n",
                "  slice_head(n = 5)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, let's look at the confusion matrix for our model\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Confusion matrix\r\n",
                "penguins_results %>% \r\n",
                "  conf_mat(species, .pred_class)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The confusion matrix shows the intersection of predicted and actual label values for each class - in simple terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
                "\n",
                "When dealing with multiple classes, it's generally more intuitive to visualize this as a heat map, like this:\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "update_geom_defaults(geom = \"tile\", new = list(color = \"black\", alpha = 0.7))\r\n",
                "# Visualize confusion matrix\r\n",
                "penguins_results %>% \r\n",
                "  conf_mat(species, .pred_class) %>% \r\n",
                "  autoplot(type = \"heatmap\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The darker squares in the confusion matrix plot indicate high numbers of cases, and you can hopefully see a diagonal line of darker squares indicating cases where the predicted and actual label are the same.\n",
                "\n",
                "Let's now calculate summary statistics for the confusion matrix.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Statistical summaries for the confusion matrix\r\n",
                "conf_mat(data = penguins_results, truth = species, estimate = .pred_class) %>% \r\n",
                "  summary()\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The tibble shows the overall metrics of how well the model performs across all three classes.\n",
                "\n",
                "Let's evaluate the ROC metrics. In the case of a multiclass classification model, a single ROC curve showing true positive rate vs false positive rate is not possible. However, you can use the rates for each class in a One vs Rest (OVR) comparison to create a ROC chart for each class.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Make a ROC_CURVE\r\n",
                "penguins_results %>% \r\n",
                "  roc_curve(species, c(.pred_Adelie, .pred_Chinstrap, .pred_Gentoo)) %>% \r\n",
                "  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\r\n",
                "  geom_abline(lty = 2, color = \"gray80\", size = 0.9) +\r\n",
                "  geom_path(show.legend = T, alpha = 0.6, size = 1.2) +\r\n",
                "  coord_equal()\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "That went down well! The model did a great job in classifying the penguins.\n",
                "\n",
                "What kind of adventure would it be, if we didn't preprocess the data for modeling?\n",
                "\n",
                "### Workflows + A different algorithm\n",
                "\n",
                "Again, just like with binary classification, you can use a workflow to apply preprocessing steps to the data before fitting it to an algorithm to train a model. Let's scale the numeric features in a transformation step before training, try a different algorithm (a support vector machine) and tune some model hyperparameters, just to show that we can!\n",
                "\n",
                "`Support Vector Machines` try to find a *hyperplane* in some feature space that \"best\" separates the classes. Please see:\n",
                "\n",
                "-   [*Support Vector Machines*](https://bradleyboehmke.github.io/HOML/svm.html), Hands-on Machine Learning with R\n",
                "\n",
                "-   *Support Vector Machines*, [An Introduction to Statistical Learning with Applications in R](https://www.statlearning.com/)\n",
                "\n",
                "-   [Support Vector Machines under the hood](https://towardsdatascience.com/support-vector-machines-under-the-hood-c609e57a4b09)\n",
                "\n",
                "-   [SVM kernels](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html), Scikit learn\n",
                "\n",
                "We'll fit a `radial basis function` support vector machine to these data and tune the `SVM cost parameter` and the `œÉ parameter` in the kernel function (The margin parameter does not apply to classification models)\n",
                "\n",
                "-   A cost argument allows us to specify the cost of a violation to the margin. When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. This *could* make the model more robust and lead to better classification. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n",
                "\n",
                "-   As `œÉ` decreases, the fit becomes more non-linear, and the model *becomes* more flexible.\n",
                "\n",
                "Both parameters can have a profound effect on the model complexity and performance.\n",
                "\n",
                "> The radial basis kernel is extremely flexible, as such, we generally start with this kernel when fitting SVMs in practice.\n",
                "\n",
                "Parameters are marked for tuning by assigning them a value of `tune()`.\n",
                "\n",
                "Also, let's try out a new succinct way of creating workflows that minimizes a lot of piping steps.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Create a model specification\r\n",
                "svm_spec <- svm_rbf(mode = \"classification\", engine = \"kernlab\",\r\n",
                "            cost = tune(), rbf_sigma = tune())\r\n",
                "\r\n",
                "\r\n",
                "# Create a workflow that encapsulates a recipe and a model\r\n",
                "svm_wflow <- recipe(species ~ ., data = penguins_train) %>% \r\n",
                "  step_normalize(all_numeric_predictors()) %>% \r\n",
                "  workflow(svm_spec)\r\n",
                "\r\n",
                "# Print out workflow\r\n",
                "svm_wflow\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Pretty neat, right ‚ú®?\n",
                "\n",
                "Now that we have specified what parameter to tune, we'll need to figure out a set of possible values to try out then choose the best.\n",
                "\n",
                "To do this, we'll create a grid! In this example, we'll work through a regular grid of hyperparameter values, try them out, and see what pair results in the best model performance.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "set.seed(2056)\r\n",
                "# Create regular grid of 6 values for each tuning parameters\r\n",
                "svm_grid <- grid_regular(parameters(svm_spec), levels = 6)\r\n",
                "\r\n",
                "# Print out some parameters in our grid\r\n",
                "svm_grid %>% \r\n",
                "  slice_head(n = 10)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Awesome! One thing about hyperparameters is that they are not learned directly from the training set. Instead, they are estimated using `simulated data sets` created from a process called `resampling`. In our previous, we used `cross-validation` resampling method. Let's try out another resampling technique: `bootstrap resampling`.\n",
                "\n",
                "Bootstrap resampling means drawing with `replacement` from our original dataset then then fit a model on that new set that `contains some duplicates`, and evaluate the model on the `data points that were not included`.\n",
                "\n",
                "Then we do that again (default behaviour is 25 boostraps but this can be changed). Okay, let's create some simulated data sets.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "set.seed(2056)\r\n",
                "# Bootstrap resampling\r\n",
                "penguins_bs <- bootstraps(penguins_train, times = 10)\r\n",
                "\r\n",
                "penguins_bs\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Model tuning via grid search.\n",
                "\n",
                "We are ready to tune! Let's use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) to fit models at all the different values we chose for each hyperparameter.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "doParallel::registerDoParallel()\r\n",
                "\r\n",
                "# Model tuning via a grid search\r\n",
                "set.seed(2056)\r\n",
                "svm_res <- tune_grid(\r\n",
                "  object = svm_wflow,\r\n",
                "  resamples = penguins_bs,\r\n",
                "  grid = svm_grid\r\n",
                ")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now that we have our tuning results, we can extract the performance metrics using `collect_metrics()`:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Obtain performance metrics\r\n",
                "svm_res %>% \r\n",
                "  collect_metrics() %>% \r\n",
                "  slice_head(n = 7)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "ü§©Let's see if we could get more by visualizing the results:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Visualize tuning metrics\r\n",
                "svm_res %>% \r\n",
                "  collect_metrics() %>% \r\n",
                "  mutate(rbf_sigma = factor(rbf_sigma)) %>% \r\n",
                "  ggplot(mapping = aes(x = cost, y = mean, color = rbf_sigma)) +\r\n",
                "  geom_line(size = 1.5, alpha = 0.7) +\r\n",
                "  geom_point(size = 2) +\r\n",
                "  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\r\n",
                "  scale_x_log10(labels = scales::label_number()) +\r\n",
                "  scale_color_viridis_d(option = \"viridis\", begin = .1)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "It seems that an SVM with an rbf_sigma of 1 and 0.01 really performed well across all candidate values of cost. The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function can help us make a clearer distinction:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Show best submodel \r\n",
                "svm_res %>% \r\n",
                "  show_best(\"accuracy\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Much better! Let's now use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values in the best sub-model:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Select best model hyperparameters\r\n",
                "best_svm <- svm_res %>% \r\n",
                "  select_best(\"accuracy\")\r\n",
                "\r\n",
                "best_svm\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Perfect! These are the values for `rbf_sigma` and `cost` that maximize accuracy for our penguins!\n",
                "\n",
                "We can now `finalize` our workflow such that the parameters we had marked for tuning by assigning them a value of `tune()` can get updated with the values from `best_svm`\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Finalize workflow\r\n",
                "final_wflow <- svm_wflow %>% \r\n",
                "  finalize_workflow(best_svm)\r\n",
                "\r\n",
                "final_wflow\r\n",
                "  \r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "That marks the end of tuning üíÉ!\n",
                "\n",
                "### The last fit\n",
                "\n",
                "Finally, let's fit this final model to the training data and evaluate how it would perform on new data using our test data. We can use the function [`last_fit()`](https://tune.tidymodels.org/reference/last_fit.html) with our finalized model; this function *fits* the finalized model on the full training data set and *evaluates* it on the testing data.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# The last fit\r\n",
                "final_fit <- last_fit(object = final_wflow, split = penguins_split)\r\n",
                "\r\n",
                "# Collect metrics\r\n",
                "final_fit %>% \r\n",
                "  collect_metrics()\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Much better ü§©! You can of course go ahead and obtain the hard class and probability predictions using `collect predictions()` and you will be well on your way to computing the confusion matrix and other summaries that come with it.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Collect predictions and make confusion matrix\r\n",
                "final_fit %>% \r\n",
                "  collect_predictions() %>% \r\n",
                "  conf_mat(truth = species, estimate = .pred_class)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Use the model with new data observations\n",
                "\n",
                "Now let's save our trained model so we can use it again later. Begin by extracting the *trained workflow* object from `final_fit` object.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Extract trained workflow\r\n",
                "penguins_svm_model <- final_fit %>% \r\n",
                "  extract_workflow()\r\n",
                "\r\n",
                "# Save workflow\r\n",
                "library(here)\r\n",
                "saveRDS(penguins_svm_model, \"penguins_svm_model.rds\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "OK, so now we have a trained model. Let's use it to predict the class of a new penguin observation:\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load model\r\n",
                "loaded_psvm_model <- readRDS(\"penguins_svm_model.rds\") \r\n",
                "\r\n",
                "# Create new tibble of observations\r\n",
                "new_obs <- tibble(\r\n",
                "  bill_length_mm = c(49.5, 38.2),\r\n",
                "  bill_depth_mm = c(18.4, 20.1),\r\n",
                "  flipper_length_mm = c(195, 190),\r\n",
                "  body_mass_g = c(3600, 3900))\r\n",
                "\r\n",
                "# Make predictions\r\n",
                "new_results <- new_obs %>% \r\n",
                "  bind_cols(loaded_psvm_model %>% \r\n",
                "              predict(new_data = new_obs))\r\n",
                "\r\n",
                "# Show predictions\r\n",
                "new_results\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Good job! A working model üêßüêß!\n",
                "\n",
                "### **Summary**\n",
                "\n",
                "Classification is one of the most common forms of machine learning, and by following the basic principles we've discussed in this notebook you should be able to train, evaluate and tune classification models with Tidymodels. It's worth spending some time investigating classification algorithms in more depth, and a good starting point is the [Tidymodels documentation](https://www.tidymodels.org/).\n"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}