{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Classification Challenge**\n",
                "\n",
                "Wine experts can identify wines from specific vineyards through smell and taste, but the factors that give different wines their individual characteristics are actually based on their chemical composition.\n",
                "\n",
                "In this challenge, you must train a classification model to analyze the chemical and visual features of wine samples and classify them based on their cultivar (grape variety).\n",
                "\n",
                "> **Citation**: The data used in this exercise was originally collected by Forina, M. et al.\n",
                ">\n",
                "> PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.\n",
                ">\n",
                "> It can be downloaded from the UCI dataset repository (Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science).\n",
                "\n",
                "### **Explore the data**\n",
                "\n",
                "Let's hit the ground running by importing a CSV file of wine data, which consists of 12 numeric features and a classification label with the following classes:\n",
                "\n",
                "-   **0** (*variety A*)\n",
                "\n",
                "-   **1** (*variety B*)\n",
                "\n",
                "-   **2** (*variety C*)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the required packages into your current R session\n",
                "suppressPackageStartupMessages({\n",
                "  library(tidyverse)\n",
                "  library(tidymodels)\n",
                "  library(here)\n",
                "  library(janitor)\n",
                "})\n",
                "\n",
                "# Read the csv file into a tibble\n",
                "wine_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/challenges/data/wine.csv\", show_col_types = FALSE)\n",
                "\n",
                "# Print the first 10 rows of the data\n",
                "wine_data %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Your challenge is to explore the data and train a classification model that achieves an overall *Recall* metric of over 0.95 (95%).\n",
                "\n",
                "Let's kick off our adventure by reformatting the data to make it easier for a model to use effectively.\n",
                "\n",
                "**Question 1.**\n",
                "\n",
                "Starting with the `wine_data`, encode the `WineVariety` column to a categorical variable (factor) with levels set as: 0, 1 ,2. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 1\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the janitor package for cleaning data\n",
                "library(janitor)\n",
                "\n",
                "# Clean data a bit\n",
                "wine_data <- wine_data %>%\n",
                "  # Encode WineVariety as category\n",
                "  mutate(WineVariety = factor(WineVariety, levels = c(0, 1, 2))) %>% \n",
                "  # Clean column names\n",
                "  clean_names()\n",
                "\n",
                "\n",
                "# View data set\n",
                "wine_data %>% \n",
                "  glimpse()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Great start! Your tibble dimensions are correct.\n",
                "\n",
                "failure_message: Almost there! Let's take a look at this again. Expected dimensions [178 14]\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "suppressPackageStartupMessages({\n",
                "  library(testthat)\n",
                "  library(ottr)\n",
                "})\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(wine_data), c(178, 14))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent. You have successfully encoded the **wine_variety** column as a factor variable with levels 0, 1, 2.\n",
                "\n",
                "failure_message: Almost there! Ensure the **wine_variety column is encoded as a factor with levels 0, 1, 2. Hint: factor(..., levels = c(...))\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('wine_variety is encoded as categorical', {\n",
                "    expect_equal(class(wine_data$wine_variety), \"factor\")\n",
                "  expect_equal(levels(wine_data$wine_variety), c(\"0\", \"1\", \"2\"))\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The goal of data exploration is to try to understand the `relationships` between its attributes; in particular, any apparent correlation between the *features* and the *label* your model will try to predict. One way of doing this is by using data visualization.\n",
                "\n",
                "Now let's compare the feature distributions for each label value.\n",
                "\n",
                "**Question 2.**\n",
                "\n",
                "We'll begin by restructuring the data such that we can easily plot our data as facets, subplots that each display one subset of the data. This is done using [`facet_wrap`](https://ggplot2.tidyverse.org/reference/facet_wrap.html)\n",
                "\n",
                "- `pivot_longer` the data (increase the number of rows and decrease the number of columns) such that all the existing column names except **wine_variety** now fall under a new column name called **`features`** and their corresponding values under a new column name **`values`**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 2\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "theme_set(theme_light())\n",
                "# Pivot data to a long format\n",
                "wine_data_long <- wine_data %>% \n",
                "    pivot_longer(!wine_variety, names_to = \"features\", values_to = \"values\")\n",
                "\n",
                "\n",
                "# Make a box plot for each predictor feature\n",
                "wine_data_long %>% \n",
                "  ggplot(mapping = aes(x = wine_variety, y = values, fill = features)) +\n",
                "  geom_boxplot() + \n",
                "  facet_wrap(~ features, scales = \"free\", ncol = 5) +\n",
                "  scale_color_viridis_d(option = \"plasma\", end = .7) +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What insights about the wine varieties have you derived from the distribution of the different features?\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Congratulations! You have successfully pivoted the data to a longer format suitable for plotting facets.\n",
                "\n",
                "failure_message: Almost there! Ensure you pivoted the exsiting columns except wine_variety to obtain two new columns **features** and **values**\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(wine_data_long), c(2314, 3))\n",
                "  expect_equal(sort(colnames(wine_data_long)), c(\"features\", \"values\", \"wine_variety\"))\n",
                "  \n",
                "  expect_equal(sort(unique(wine_data_long$features)) %>% paste(collapse = \", \"), \"alcalinity, alcohol, ash, color_intensity, flavanoids, hue, magnesium, malic_acid, nonflavanoids, od280_315_of_diluted_wines, phenols, proanthocyanins, proline\")\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split the data for training and validation\n",
                "\n",
                "It is best practice to hold out some of your data for **testing** in order to get a better estimate of how your models will perform on new data by comparing the predicted labels with the already known labels in the test set.\n",
                "\n",
                "**Question 3.**\n",
                "\n",
                "In this section:\n",
                "\n",
                "- Make a split specification of `wine_data` such that `70%` goes to training and the rest goes to testing. Save this to a variable name `wine_split`\n",
                "\n",
                "- Extract the training and testing sets from `wine_split` and save them in `wine_train` and `wine_test` variable names respectively.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 3\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the Tidymodels packages\n",
                "library(tidymodels)\n",
                "\n",
                "\n",
                "\n",
                "# Split data into 70% for training and 30% for testing\n",
                "set.seed(2056)\n",
                "wine_split <- wine_data %>% \n",
                "  initial_split(prop = 0.70)\n",
                "\n",
                "\n",
                "# Extract the data in each split\n",
                "wine_train <- training(wine_split)\n",
                "wine_test <- testing(wine_split)\n",
                "\n",
                "\n",
                "# Print the number of cases in each split\n",
                "cat(\"Training cases: \", nrow(wine_train), \"\\n\",\n",
                "    \"Test cases: \", nrow(wine_test), sep = \"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! You have successfully split the data and extracted the training (70%) and testing sets (30%).\n",
                "\n",
                "failure_message: Almost there. Let's have a look at this again.Ensure that the splitting specification dictates that 70% of the data should go to training and the rest to testing.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(wine_train), c(124, 14))\n",
                "  expect_equal(dim(wine_test), c(54, 14))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data preprocessing with recipes\n",
                "\n",
                "Now that we have a set of training features and corresponding training label (`wine_variety`), we can fit a multiclass classification algorithm to the data to create a model.\n",
                "\n",
                "`parsnip::multinom_reg()` defines a model that uses linear predictors to predict multiclass data using the multinomial distribution.\n",
                "\n",
                "\n",
                "**Question 4.**\n",
                "\n",
                "In this section:\n",
                "\n",
                "- Create a multinomial model specification `mr_spec` which uses `nnet` package as its engine and whose mode is set to `classifcation`. This model has 1 tuning parameters: `penalty`: Amount of Regularization. The best way to estimate this value is to `tune` it, but for now, set it to an arbitrary value, say `1`.\n",
                "\n",
                "- Create a recipe, `wine_recipe`, with `wine_variety` as the outcome and the rest as predictors, and a step that specifies that all the numeric predictors should be normalized.\n",
                "\n",
                "- Bundle the model specification and recipe into a workflow, `mr_wflow`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 4\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Specify a multinomial regression via nnet\n",
                "mr_spec <- multinom_reg(\n",
                "  penalty = 1,\n",
                "  engine = \"nnet\",\n",
                "  mode = \"classification\"\n",
                ")\n",
                "\n",
                "\n",
                "# Create a recipe that specifies that predictors should be on the same scale\n",
                "wine_recipe <- recipe(wine_variety ~ ., data = wine_train) %>% \n",
                "  step_normalize(all_numeric_predictors())\n",
                "\n",
                "# Bundle recipe and model specification into a workflow\n",
                "mr_wflow <- workflow(preprocessor = wine_recipe, spec = mr_spec)\n",
                "\n",
                "# Print out workflow\n",
                "mr_wflow\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent! Your model specification is looking great!\n",
                "\n",
                "failure_message: Let's have a look at this again. Ensure you have set your engine to **nnet** and the mode to **classification**.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('the model specification is correct', {\n",
                "  expect_equal(mr_spec$mode, \"classification\")\n",
                "  expect_equal(mr_spec$engine, \"nnet\")\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent! Your recipe is looking great!\n",
                "\n",
                "failure_message: Let's have a look at this again. Ensure you have set the **wine_variety** variable as the outcome and the rest of the variables as predictors, and a step that specifies that all the numeric predictors should be normalized.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('recipe specification is correct', {\n",
                "  expect_output(invisible(print(wine_recipe)), \"Recipe\\n\\nInputs:\\n\\n\\nOperations:\\n\\nCentering and scaling for all_numeric_predictors()\", fixed = TRUE)\n",
                "  \n",
                "  expect_equal(summary(wine_recipe) %>% filter(variable == \"wine_variety\") %>% pull(role), \"outcome\")\n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent! Your workflow is all set up!\n",
                "\n",
                "failure_message: Let's have a look at this again. Perhaps we forgot to add something e.g add_model(...)?.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('workflow specification is correct', {\n",
                "  expect_output(print(mr_wflow), \"== Workflow ====================================================================\\nPreprocessor: Recipe\\nModel: multinom_reg()\\n\\n-- Preprocessor ----------------------------------------------------------------\\n1 Recipe Step\\n\\n* step_normalize()\\n\\n-- Model -----------------------------------------------------------------------\\nMultinomial Regression Model Specification (classification)\\n\\n\", fixed = TRUE)\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Good job!\n",
                "\n",
                "After a workflow has been *specified*, a model can be `trained` using the [`fit()`](https://tidymodels.github.io/parsnip/reference/fit.html) function.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit a workflow object\n",
                "mr_wflow_fit <- mr_wflow %>% \n",
                "  fit(data = wine_train)\n",
                "\n",
                "# Print wf object\n",
                "mr_wflow_fit\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluate model performance\n",
                "\n",
                "Good job! We now have a trained workflow. The workflow print out shows the coefficients learned during training.\n",
                "\n",
                "**Question 5.**\n",
                "\n",
                "This allows us to use the model trained by this workflow to predict labels and their corresponding class probabilities for the test set.\n",
                "\n",
                "In this section:\n",
                "\n",
                "- Obtain a tibble containing the actual wine varieties of the test set, the hard class wine variety predictions and their corresponding probability predictions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 5\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Obtain predictions on the test set\n",
                "results <- mr_wflow_fit %>% \n",
                "  augment(wine_test) \n",
                "\n",
                "# Print the results\n",
                "results %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! You have successfully used the trained model to make hard class and probability predictions on the test set.\n",
                "\n",
                "failure_message: Let's have a look at this again! Ensure you have used your trained model to make hard class and probability predictions on the test set. Also add the true outcomes to the results. \n",
                "Hints: augment or predict + bind_cols functions.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('results contain true values, hard class predictions and probabilities', {\n",
                "  expect_equal(sum(c(\"wine_variety\", \".pred_class\", \".pred_0\", \".pred_1\", \".pred_2\") %in% names(results)), 5)\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Awesome! Time to evaluate how our model performed. Let's look at the confusion matrix for our model\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "results %>% \n",
                "  conf_mat(truth = wine_variety, estimate = .pred_class)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The confusion matrix shows the intersection of predicted and actual label values for each class - in simple terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
                "\n",
                "When dealing with multiple classes, it's generally more intuitive to visualize this as a heat map, like this:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "update_geom_defaults(geom = \"tile\", new = list(color = \"black\", alpha = 0.7))\n",
                "# Visualize confusion matrix\n",
                "results %>% \n",
                "  conf_mat(wine_variety, .pred_class) %>% \n",
                "  autoplot(type = \"heatmap\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The darker squares in the confusion matrix plot indicate high numbers of cases, and you can hopefully see a diagonal line of darker squares indicating cases where the predicted and actual label are the same.\n",
                "\n",
                "\n",
                "**Question 6.**\n",
                "\n",
                "The confusion matrix is helpful since it gives rise to other metrics that can help us better evaluate the performance of a classification model. Let’s go through some of them:\n",
                "\n",
                "`Precision: TP/(TP + FP)` defined as the proportion of predicted positives that are actually positive. \n",
                "\n",
                "`Recall: TP/(TP + FN)` defined as the proportion of positive results out of the number of samples which were actually positive.\n",
                "\n",
                "\n",
                "`Accuracy: TP + TN/(TP + TN + FP + FN)` The percentage of labels predicted accurately for a sample.\n",
                "\n",
                "\n",
                "In this section, using the `yardstick` package, obtain the following metrics:\n",
                "\n",
                "- Accuracy\n",
                "- Precision\n",
                "- Recall\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 6\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summaries for the confusion matrix\n",
                "accuracy = accuracy(data = results, truth = wine_variety, estimate = .pred_class)\n",
                "\n",
                "precision = precision(data = results, truth = wine_variety, estimate = .pred_class)\n",
                "\n",
                "recall = recall(data = results, truth = wine_variety, estimate = .pred_class)\n",
                "\n",
                "# Simpler way\n",
                "conf_mat(data = results, truth = wine_variety, estimate = .pred_class) %>% \n",
                "  summary() %>% \n",
                "  filter(.metric %in% c(\"accuracy\", \"recall\", \"precision\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('accuracy is correct', {\n",
                "  expect_equal(accuracy$.metric, \"accuracy\")\n",
                "  expect_equal(accuracy$.estimator, \"multiclass\")\n",
                "  expect_true(is.numeric(accuracy$.estimate))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('precision is correct', {\n",
                "  expect_equal(precision$.metric, \"precision\")\n",
                "  expect_equal(precision$.estimator, \"macro\")\n",
                "  expect_true(is.numeric(precision$.estimate))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('recall is correct', {\n",
                "  expect_equal(recall$.metric, \"recall\")\n",
                "  expect_equal(recall$.estimator, \"macro\")\n",
                "  expect_true(is.numeric(recall$.estimate))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What do you think of these metrics?\n",
                "\n",
                "Let's now evaluate the ROC metrics. In the case of a multiclass classification model, a single ROC curve showing true positive rate vs false positive rate is not possible. However, you can use the rates for each class in a One vs Rest (OVR) comparison to create a ROC chart for each class as below:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make a ROC_CURVE\n",
                "results %>% \n",
                "  roc_curve(wine_variety, c(.pred_0, .pred_1, .pred_2)) %>% \n",
                "  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n",
                "  geom_abline(lty = 2, color = \"gray80\", size = 0.9) +\n",
                "  geom_path(show.legend = T, alpha = 0.6, size = 1.2) +\n",
                "  coord_equal()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To quantify the ROC performance, you can calculate an aggregate area under the curve score that is averaged across all of the OVR curves.\n",
                "\n",
                "**Question7**\n",
                "In this section:\n",
                "- Quantify the ROC performance by calculating the aggregate area under the curve score that is averaged across all of the OVR curves.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 7\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC_AUC\n",
                "auc <- results %>% \n",
                "  roc_auc(wine_variety, c(.pred_0, .pred_1, .pred_2))\n",
                "\n",
                "auc\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "success_message: Good job! You have successfully quantified the ROC performance.\n",
                "\n",
                "failure_message: Almost there. Let's have a look at this again. Ensure you are calculating the roc_auc performance.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('recall is correct', {\n",
                "  expect_equal(auc$.metric, \"roc_auc\")\n",
                "  expect_equal(auc$.estimator, \"hand_till\")\n",
                "  expect_true(is.numeric(auc$.estimate))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Overall, did the model do a great job in classifying the wine varieties? Be sure to try out a different classification algorithm.\n",
                "\n",
                "### **Use the model with new data observation**\n",
                "\n",
                "When you're happy with your model's predictive performance, save it and then use it to predict classes for the following two new wine samples:\n",
                "\n",
                "-   \\[13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285\\]\n",
                "\n",
                "-   \\[12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520\\]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(here)\n",
                "# Save trained workflow\n",
                "saveRDS(mr_wflow_fit, \"wine_mr_model.rds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can load it whenever we need it, and use it to predict labels for new data. This is often called *`scoring`* or *`inferencing`*.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a tibble for the new wine samples\n",
                "new_wines <- c(13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285) %>% \n",
                "  rbind(c(12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520)) %>% \n",
                "  as_tibble()\n",
                "names(new_wines) = names(wine_data)[1:13]\n",
                "\n",
                "new_wines\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the model into the current R session\n",
                "loaded_model <- readRDS(\"wine_mr_model.rds\")\n",
                "\n",
                "# Make predictions\n",
                "predictions <- loaded_model %>% \n",
                "  augment(new_data = new_wines)\n",
                "\n",
                "predictions\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That's it for now. Congratulations on finishing this challenge notebook! The adventure does not stop there, you could try tuning that hyperparameter, or try out a new model entirely.\n",
                "\n",
                "\n",
                "Happy Learning,\n",
                "\n",
                "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(here)\n",
                "library(rmd2jupyter)\n",
                "rmd2jupyter(\"03_Classification_Solution_Ottr.Rmd\")\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
