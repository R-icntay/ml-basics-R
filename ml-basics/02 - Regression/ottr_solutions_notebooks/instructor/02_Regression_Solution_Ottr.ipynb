{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regression Challenge\n",
                "\n",
                "Predicting the selling price of a residential property depends on a number of factors, including the property age, availability of local amenities, and location.\n",
                "\n",
                "In this challenge, you will use a dataset of real estate sales transactions to predict the price-per-unit of a property based on its features. The price-per-unit in this data is based on a unit measurement of 3.3 square meters.\n",
                "\n",
                "> **Citation**: The data used in this exercise originates from the following study:\n",
                ">\n",
                "> *Yeh, I. C., & Hsu, T. K. (2018). Building real estate valuation models with comparative approach through case-based reasoning. Applied Soft Computing, 65, 260-271.*\n",
                ">\n",
                "> It was obtained from the UCI dataset repository (Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).\n",
                "\n",
                "## Review the data\n",
                "\n",
                "Let's hit the ground running by importing the data and viewing the first few rows.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the core tidyverse and tidymodels in your current R session\n",
                "suppressPackageStartupMessages({\n",
                " library(tidyverse)\n",
                " library(tidymodels)\n",
                "})\n",
                "\n",
                "# Read the csv file into a tibble\n",
                "estate_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/challenges/data/real_estate.csv\", show_col_types = FALSE)\n",
                "\n",
                "# Print the first 10 rows of the data\n",
                "estate_data %>% \n",
                " slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The data consists of the following variables:\n",
                "\n",
                "-   **transaction_date** - the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)\n",
                "\n",
                "-   **house_age** - the house age (in years)\n",
                "\n",
                "-   **transit_distance** - the distance to the nearest light rail station (in meters)\n",
                "\n",
                "-   **local_convenience_stores** - the number of convenience stores within walking distance\n",
                "\n",
                "-   **latitude** - the geographic coordinate, latitude\n",
                "\n",
                "-   **longitude** - the geographic coordinate, longitude\n",
                "\n",
                "-   **price_per_unit** house price of unit area (3.3 square meters)\n",
                "\n",
                "## **Train a Regression Model**\n",
                "\n",
                "Your challenge is to explore and prepare the data, identify predictive features that will help predict the **price_per_unit** label, and train a regression model that achieves the lowest **Root Mean Square Error** (RMSE) you can achieve (which must be less than **7**) when evaluated against a test subset of data.\n",
                "\n",
                "### View the label distribution\n",
                "\n",
                "Let's start our analysis of the data by examining a few key descriptive statistics. We can use the `summarytools::descr()` function to neatly and quickly summarize the numeric features as well as the **rentals** label column.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load summary tools library\n",
                "library(summarytools)\n",
                "\n",
                "# Obtain summary stats for feature and label columns\n",
                "estate_data %>% \n",
                "  # Summary stats\n",
                "  descr(order = \"preserve\",\n",
                "        stats = c('mean', 'sd', 'min', 'q1', 'med', 'q3', 'max'),\n",
                "        round.digits = 6)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The statistics reveal some information about the distribution of the data in each of the numeric fields, including the number of observations (there are 414 records), the mean, standard deviation, minimum and maximum values, and the quantile values (the threshold values for 25%, 50% - which is also the median, and 75% of the data).\n",
                "\n",
                "From this, we can see that the mean number of price per unit is around 38. There's a comparatively `small standard deviation`, indicating `not much variance` in the prices per unit.\n",
                "\n",
                "We might get a clearer idea of the distribution of price values by visualizing the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(patchwork)\n",
                "\n",
                "# Plot a histogram\n",
                "theme_set(theme_light())\n",
                "\n",
                "hist_plt <- estate_data %>% \n",
                "  ggplot(mapping = aes(x = price_per_unit)) +\n",
                "  geom_histogram(bins = 100, fill = \"midnightblue\", alpha = 0.7) +\n",
                "  \n",
                "  # Add lines for mean and median\n",
                "  geom_vline(aes(xintercept = mean(price_per_unit), color = 'Mean'), linetype = \"dashed\", size = 1.3) +\n",
                "  geom_vline(aes(xintercept = median(price_per_unit), color = 'Median'), linetype = \"dashed\", size = 1.3 ) +\n",
                "  xlab(\"\") +\n",
                "  ylab(\"Frequency\") +\n",
                "  scale_color_manual(name = \"\", values = c(Mean = \"red\", Median = \"yellow\")) +\n",
                "  theme(legend.position = c(0.9, 0.9), legend.background = element_blank())\n",
                "\n",
                "# Plot a box plot\n",
                "box_plt <- estate_data %>% \n",
                "  ggplot(aes(x = price_per_unit, y = 1)) +\n",
                "  geom_boxplot(fill = \"#E69F00\", color = \"gray23\", alpha = 0.7) +\n",
                "    # Add titles and labels\n",
                "  xlab(\"Price_per_unit\")+\n",
                "  ylab(\"\")\n",
                "\n",
                "\n",
                "# Combine plots using patchwork syntax\n",
                "(hist_plt / box_plt) +\n",
                "  plot_annotation(title = 'Price Distribution',\n",
                "                  theme = theme(\n",
                "                    plot.title = element_text(hjust = 0.5)))\n",
                "  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What can you observe from the boxplot? Yes, outliers.\n",
                "\n",
                "### Remove outliers\n",
                "\n",
                "We are now set to begin writing some code ourselves. Let's begin by dealing with ouliers. An outlier is a data point that differs significantly from other observations.\n",
                "\n",
                "**Question 1.**\n",
                "\n",
                "Starting with the `estate_data` dataset, `filter` to create a subset that contains observations where `price_per_unit` is less than `70`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 1\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Narrow down to observations whose price_per_unit is less than 70\n",
                "estate_data <- estate_data %>% \n",
                "  filter(price_per_unit < 70)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Great start! Your tibble dimensions are correct.\n",
                "\n",
                "failure_message: Almost there! Ensure you have filtered correctly to obtain a subset whose observations of `price_per_unit` is less than `70`. Expected dimensions [408 7]\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "suppressPackageStartupMessages({\n",
                "  library(testthat)\n",
                "  library(ottr)\n",
                "})\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(estate_data), c(408, 7))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent. You have successfully created a subset whose observations of price_per_unit is less than 70.\n",
                "\n",
                "failure_message: Let's give this another try. Ensure your subset contains observations where **price_per_unit** is less than 70\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('the range of values for price per unit is within 7.6 and 69.7', {\n",
                "    expect_equal(range(estate_data$price_per_unit), c(7.6, 69.7))\n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's take a look at the distribution without the outliers.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot a histogram\n",
                "theme_set(theme_light())\n",
                "hist_plt <- estate_data %>% \n",
                "  ggplot(mapping = aes(x = price_per_unit)) + \n",
                "  geom_histogram(bins = 100, fill = \"midnightblue\", alpha = 0.7) +\n",
                "  \n",
                "  # Add lines for mean and median\n",
                "  geom_vline(aes(xintercept = mean(price_per_unit), color = 'Mean'), linetype = \"dashed\", size = 1.3) +\n",
                "  geom_vline(aes(xintercept = median(price_per_unit), color = 'Median'), linetype = \"dashed\", size = 1.3 ) +\n",
                "  xlab(\"\") +\n",
                "  ylab(\"Frequency\") +\n",
                "  scale_color_manual(name = \"\", values = c(Mean = \"red\", Median = \"yellow\")) +\n",
                "  theme(legend.position = c(0.9, 0.9), legend.background = element_blank())\n",
                "\n",
                "# Plot a box plot\n",
                "box_plt <- estate_data %>% \n",
                "  ggplot(aes(x = price_per_unit, y = 1)) +\n",
                "  geom_boxplot(fill = \"#E69F00\", color = \"gray23\", alpha = 0.7) +\n",
                "    # Add titles and labels\n",
                "  xlab(\"Price_per_unit\")+\n",
                "  ylab(\"\")\n",
                "\n",
                "\n",
                "# Combine plots using patchwork syntax\n",
                "(hist_plt / box_plt) +\n",
                "  plot_annotation(title = 'Price Distribution',\n",
                "                  theme = theme(\n",
                "                    plot.title = element_text(hjust = 0.5)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Much better! What can you say about the distribution of the price?\n",
                "\n",
                "### View numeric correlations\n",
                "\n",
                "We can now start to look for relationships between the *features* and the *label* we want to be able to predict.\n",
                "\n",
                "The *correlation* statistic, *r*, is a value between -1 and 1 that indicates the strength of a linear relationship.\n",
                "\n",
                "For the numeric features, we can create scatter plots that show the intersection of feature and label values.\n",
                "\n",
                "\n",
                "**Question 2.**\n",
                "\n",
                "Starting with the `estate_data` dataset, in a piped sequence:\n",
                "- `pivot_longer` the data (increase the number of rows and decrease the number of columns) such that all the existing column names except price_per_unit now fall under a new column name called **`features`** and their corresponding values under a new column name **`values`**\n",
                "\n",
                "- group the data by `features`\n",
                "\n",
                "- add a new column `corr_coef` which calculates the correlation between `values` and `price_per_unit` (hint: the function used for calculating correlation in R is `cor()`)  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 2\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pivot numeric features to a long format\n",
                "numeric_features_long <- estate_data %>% \n",
                "  pivot_longer(!price_per_unit, names_to = \"features\", values_to = \"values\") %>%\n",
                "  group_by(features) %>% \n",
                "  mutate(corr_coef = cor(values, price_per_unit),\n",
                "         features = paste(features, ' vs price, r = ', round(corr_coef, 2), sep = '')) %>% \n",
                "  ungroup()\n",
                "\n",
                "# Print the first few rows of the data\n",
                "numeric_features_long %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Great start! Your tibble dimensions and corresponding columns are correct.\n",
                "\n",
                "failure_message: Almost there! Let's give this another shot.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(numeric_features_long), c(2448, 4))\n",
                "  expect_equal(sort(colnames(numeric_features_long)), c(\"corr_coef\", \"features\", \"price_per_unit\", \"values\"))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent. You have successfully pivoted the tibble and found the correlation between the existing numeric column values and the price per unit.\n",
                "\n",
                "failure_message: Let's give this another try. Ensure you have correctly pivoted the data to obtain two new columns **features** and **values** and then grouped by **features** and then added a new column **corr_coef** which is the correlation between **values** and **price_per_unit**. Lastly don't forget to ungroup :).\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('the correlation coefficients are correct', {\n",
                "    expect_equal(round(range(numeric_features_long$corr_coef), 7), c(-0.7087782, 0.6101017))\n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fantastic! Now let's use a scatter plot to investigate whether there is any linear relationship between our predictors and outcome variables.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot a scatter plot for each feature\n",
                "numeric_features_long %>% \n",
                "  ggplot(aes(x = values, y = price_per_unit, color = features)) +\n",
                "  geom_point(alpha = 0.7, show.legend = F) +\n",
                "  facet_wrap(~ features, scales = 'free') +\n",
                "  paletteer::scale_color_paletteer_d(\"ggthemes::excel_Parallax\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Take a moment and go through the scatter plot. How does the correlation between these features and the price vary?\n",
                "\n",
                "### View categorical features\n",
                "\n",
                "Now let's compare the categorical features to the label. We'll do this by creating box plots that show the distribution of rental counts for each category.\n",
                "\n",
                "**transaction_date** and **local_convenience_stores** seem to be discrete values - so might work better if treated as categorical features. Let' get right into it.\n",
                "\n",
                "**Question 3.**\n",
                "\n",
                "Starting with the `estate_data` dataset, in a piped sequence:\n",
                "\n",
                "- only keep columns `transaction_date`, `local_convenience_stores` and `price_per_unit`\n",
                "\n",
                "- encode columns `transaction_date` and `local_convenience_stores` as categorical (factor)\n",
                "\n",
                "- `pivot_longer` the data (increase the number of rows and decrease the number of columns) such that all the existing column names except price_per_unit now fall under a new column name called **`features`** and their corresponding values under a new column name **`values`**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 3\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pivot categorical features to a long format\n",
                "cat_features_long <- estate_data %>% \n",
                "  select(transaction_date, local_convenience_stores, price_per_unit) %>% \n",
                "  # Encode features from numeric to categorical\n",
                "  mutate(across(c(transaction_date, local_convenience_stores), factor)) %>% \n",
                "  pivot_longer(!price_per_unit, names_to = \"features\", values_to = \"values\") \n",
                "\n",
                "# Print some observations\n",
                "cat_features_long %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! Your tibble dimensions and corresponding columns are correct.\n",
                "\n",
                "failure_message: Almost there! Ensure you have selected columns transaction_date, local_convenience_stores and price_per_unit, and then pivoted the exsiting columns except price_per_unit to obtain two new columns **features** and **values**\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(cat_features_long), c(816, 3))\n",
                "  expect_equal(sort(colnames(cat_features_long)), c(\"features\", \"price_per_unit\", \"values\"))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Congratulations! You have successfully selected the desired columns, encoded some of them as categorical and restructured the data to a longer format.\n",
                "\n",
                "failure_message: Almost there! Ensure you have selected columns transaction_date, local_convenience_stores and price_per_unit, and then encoded transaction_date & local_convenience_stores as categorical, and then pivoted the data correctly.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data contains the correct observations', {\n",
                "  expect_equal(sort(unique(cat_features_long$features)), c(\"local_convenience_stores\", \"transaction_date\"))\n",
                "  expect_equal(class(cat_features_long$values), \"factor\")\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Perfect! Now, for our categorical features, boxplots can be a great way of visualising how the price per unit varies within the levels of the categorical feature.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot a box plot for each feature\n",
                "cat_features_long %>%\n",
                "  ggplot() +\n",
                "  geom_boxplot(aes(x = values, y = price_per_unit, fill = features), alpha = 0.7, show.legend = F) +\n",
                "  facet_wrap(~ features, scales = 'free') +\n",
                "  scale_fill_viridis_d() +\n",
                "  theme(panel.grid = element_blank(),\n",
                "        axis.text.x = element_text(angle = 90))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Take a moment and interpret the graphics. How does the price vary with these features?\n",
                "\n",
                "## Split the data into training and test sets.\n",
                "\n",
                "Now that we've explored the data, it's time to use it to train a regression model that uses the features we've identified as `potentially predictive` to predict the **price_per_unit** label.\n",
                "\n",
                "**transaction_date** doesn't seem to be very predictive, so we'll omit it.\n",
                "\n",
                "Let's begin by splitting the data set such that some goes to training and some goes for validation. This enables us to evaluate how well the model performs in order to get a better estimate of how your models will perform on new data.\n",
                "\n",
                "\n",
                "**Question 4.**\n",
                "\n",
                "In this section:\n",
                "\n",
                "- Make a split specification of `estate_data` such that `70%` goes to training and the rest goes to testing. Save this to a variable name `estate_split`\n",
                "\n",
                "- Extract the training and testing sets from `estate_split` and save them in `estate_train` and `estate_test` variable names respectively.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 4\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set seed to ensure reproducibility and consitency of outputs\n",
                "set.seed(2056)\n",
                "\n",
                "# Load the tidymodels package\n",
                "library(tidymodels)\n",
                "\n",
                "# Split 70% of the data for training and the rest for tesing\n",
                "estate_split <- estate_data %>% \n",
                "  initial_split(prop = 0.7)\n",
                "\n",
                "# Extract the data in each split\n",
                "estate_train <- training(estate_split)\n",
                "estate_test <- testing(estate_split)\n",
                "\n",
                "# Print the number of observations in each split\n",
                "cat(\"Training Set\", nrow(estate_train), \"rows\",\n",
                "    \"\\nTest Set\", nrow(estate_test), \"rows\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! You have successfully split the data and extracted the training (70%) and testing sets (30%).\n",
                "\n",
                "failure_message: Almost there. Let's have a look at this again.Ensure that the splitting specification dictates that 70% of the data should go to training and the rest to testing.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('data dimensions correct', {\n",
                "  expect_equal(dim(estate_train), c(285, 7))\n",
                "  expect_equal(dim(estate_test), c(123, 7))\n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Great progress!\n",
                "\n",
                "\n",
                "## Train a regression model\n",
                "\n",
                "### Preprocess data using recipes\n",
                "\n",
                "Often before fitting a model, we may want to reformat the predictor values to make them easier for a model to use effectively. This includes transformations and encodings of the data to best represent their important characteristics. In R,this is done using a `recipe`.\n",
                "\n",
                "A recipe is an object that defines a series of steps for data processing.\n",
                "\n",
                "**Question 5.**\n",
                "\n",
                "In this section, specify a recipe, `estate_recipe`, that will:\n",
                "\n",
                "-   Remove the `transaction_date` feature\n",
                "\n",
                "-   Transform `local_convenience_stores` feature into categorical (factor)\n",
                "\n",
                "-   Center and scale all numeric predictors\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 5\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a preprocessing recipe\n",
                "estate_recipe <- recipe(price_per_unit ~ ., data = estate_train) %>%\n",
                "  # Specify the removal of a variable\n",
                "  step_rm(transaction_date) %>% \n",
                "  # Specify the encoding of local_convenience_stores as categorical\n",
                "  step_mutate(\n",
                "    local_convenience_stores = factor(local_convenience_stores)) %>%\n",
                "  # Specify the normalization of numeric features\n",
                "  step_normalize(all_numeric_predictors())\n",
                " \n",
                "# Print recipe\n",
                "estate_recipe\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Good job. You have correctly specified a recipe that will Remove the `transaction_date` feature, Transform `local_convenience_stores` feature into categorical (factor) and then Center and scale all numeric predictors.\n",
                "\n",
                "failure_message: Almost there. Ensure your recipe specification will Remove the `transaction_date` feature, Transform `local_convenience_stores` feature into categorical (factor) and then Center and scale all numeric predictors.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('recipe specification is correct', {\n",
                "  \n",
                "  # Test for step_rm\n",
                "  expect_equal(attr(estate_recipe[[\"steps\"]][[1]], \"class\"), c(\"step_rm\",\"step\"))\n",
                "  expect_equal(as_label(estate_recipe[[\"steps\"]][[1]][[\"terms\"]][[1]]), \"transaction_date\")\n",
                "  \n",
                "  # Test for step_mutate\n",
                "  expect_equal(attr(estate_recipe[[\"steps\"]][[2]], \"class\"), c(\"step_mutate\",\"step\"))\n",
                "  expect_equal(as_label(estate_recipe[[\"steps\"]][[2]][[\"inputs\"]][[\"local_convenience_stores\"]]), \"factor(local_convenience_stores)\")\n",
                "  \n",
                "  # Test for step_normalize\n",
                "  expect_equal(attr(estate_recipe[[\"steps\"]][[3]], \"class\"), c(\"step_normalize\",\"step\"))\n",
                "  expect_equal(as_label(estate_recipe[[\"steps\"]][[3]][[\"terms\"]][[1]]), \"all_numeric_predictors()\")\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fantastic! We have the data processing in order. Now, let's make a model specification. In this solution, we'll try out a random forest model which applies an averaging function to multiple decision tree models for a better overall model.\n",
                "\n",
                "**Question 6.**\n",
                "\n",
                "Create a random forest model specification, `rf_spec`, which uses the `randomForest` package as its engine and then set the mode to `regression`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 6\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a random forest model specification\n",
                "rf_spec <- rand_forest() %>% \n",
                "  set_engine('randomForest') %>% \n",
                "  set_mode('regression')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Excellent! Your model specification is looking great!\n",
                "\n",
                "failure_message: Let's have a look at this again. Ensure you have set your engine to **randomForest** and the mode to **regression**.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('the model specification is correct', {\n",
                "  expect_equal(rf_spec$mode, \"regression\")\n",
                "  expect_equal(rf_spec$engine, \"randomForest\")\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create a modeling workflow\n",
                "\n",
                "The **workflows** package allows the user to bind modeling and preprocessing objects together. You can then fit the entire workflow to the data, so that the model encapsulates all of the preprocessing steps as well as the algorithm.\n",
                "\n",
                "**Question 7.**\n",
                "\n",
                "Components of a `workflow()` go together like LEGO blocks. In this section, create a workflow container and then add the preprocessing information from our recipe and then add the model specification to be trained.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 7\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a workflow that bundles a recipe and model specification\n",
                "rf_workflow <- workflow() %>% \n",
                "  add_recipe(estate_recipe) %>% \n",
                "  add_model(rf_spec)\n",
                "\n",
                "# Print workflow\n",
                "rf_workflow\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('workflow specification is correct', {\n",
                "  \n",
                "  # Test for step_rm\n",
                "  expect_equal(attr(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[1]], \"class\"), c(\"step_rm\",\"step\"))\n",
                "  expect_equal(as_label(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[1]][[\"terms\"]][[1]]), \"transaction_date\")\n",
                "  \n",
                "  # Test for step_mutate\n",
                "  expect_equal(attr(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[2]], \"class\"), c(\"step_mutate\",\"step\"))\n",
                "  expect_equal(as_label(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[2]][[\"inputs\"]][[\"local_convenience_stores\"]]), \"factor(local_convenience_stores)\")\n",
                "  \n",
                "  # Test for step_normalize\n",
                "  expect_equal(attr(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[3]], \"class\"), c(\"step_normalize\",\"step\"))\n",
                "  expect_equal(as_label(rf_workflow[[\"pre\"]][[\"actions\"]][[\"recipe\"]][[\"recipe\"]][[\"steps\"]][[3]][[\"terms\"]][[1]]), \"all_numeric_predictors()\")\n",
                "  \n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have everything (recipe + model specification) wrapped together nicely in a workflow, we are ready to train a model. Workflows have a `fit()` method that can be used to train a model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For reproducibility\n",
                "set.seed(2056)\n",
                "\n",
                "# Train a random forest model\n",
                "rf_workflow_fit <- rf_workflow %>% \n",
                "  fit(data = estate_train)\n",
                "\n",
                "# Print out the fitted workflow\n",
                "rf_workflow_fit\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Excellent! So we now have a trained random forest model; but is it any good? Let's evaluate its performance! We'll do this by making predictions on the **test data** and then evaluate some performance metrics based on the actual outcomes. \n",
                "\n",
                "\n",
                "**Question 8.**\n",
                "\n",
                "- We'll evaluate the model performance based on the `rmse` and `rsq` metrics. Use the `metric_set()` function to combine these metric functions together into a new function, `eval_metrics`, that calculates all of them at once.\n",
                "\n",
                "- Generate predictions for the test data and then bind them to the test set. Rename the column containing predictions from `.pred` to `predictions`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    BEGIN QUESTION\n",
                "    name: Question 8\n",
                "    manual: false\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a metric set\n",
                "eval_metrics <- metric_set(rmse, rsq)\n",
                "\n",
                "\n",
                "# Make and bind predictions to test data \n",
                "results <- rf_workflow_fit %>% \n",
                "  augment(new_data = estate_test) %>% \n",
                "  rename(predictions = .pred)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! You have successfully made predictions and binded them to the test set.\n",
                "\n",
                "failure_message: Almost there! Generate predictions for the test data and then bind them to the test set. Hints: augment or predict + bind_cols functions. Also don't forget to rename your .pred column.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "## Test ##\n",
                "test_that('the model specification is correct', {\n",
                "  expect_equal(dim(results), c(123, 8))\n",
                "  expect_equal(sort(colnames(results)), c(\"house_age\", \"latitude\", \"local_convenience_stores\", \"longitude\", \"predictions\", \"price_per_unit\", \"transaction_date\", \"transit_distance\"))\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Awesome work! You have just used your trained model to make predictions on the test set.\n",
                "\n",
                "How well did the model predict the prices per unit? Let's find out by looking at the metrics.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the model\n",
                "rf_metrics <- eval_metrics(data = results,\n",
                "                           truth = price_per_unit,\n",
                "                           estimate = predictions)\n",
                "\n",
                "\n",
                "# Plot predicted vs actual\n",
                "rf_plt <- results %>% \n",
                "  ggplot(mapping = aes(x = price_per_unit, y = predictions)) +\n",
                "  geom_point(color = 'darkorchid', size = 1.6) +\n",
                "  # overlay regression line\n",
                "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
                "  ggtitle(\"Price per unit predictions\") +\n",
                "  xlab(\"Actual Labels\") +\n",
                "  ylab(\"Predicted Labels\") +\n",
                "  theme(plot.title = element_text(hjust = 0.5))\n",
                "\n",
                "# Return evaluations\n",
                "list(metrics = rf_metrics, evaluation_plot = rf_plt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How do you think the model performed. What do the values for `rsq` and `rmse` tell you? You can refer to the corresponding module for this notebook to help answer these questions in case you are stuck.\n",
                "\n",
                "## Use the Trained Model\n",
                "\n",
                "Save your trained model, and then use it to predict the price-per-unit for the following real estate transactions:\n",
                "\n",
                "| **transaction_date** | **house_age** | **transit_distance** | **local_convenience_stores** | **latitude** | **longitude** |\n",
                "|---------------------|----------------|--------------|--------|-------|------|\n",
                "| 2013.167             | 16.2          | 289.3248             | 5                            | 24.98203     | 121.54348     |\n",
                "| 2013.000             | 13.6          | 4082.015             | 0                            | 24.94155     | 121.50381     |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(here)\n",
                "# Save trained workflow\n",
                "saveRDS(rf_workflow_fit, \"rf_price_model.rds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can load it whenever we need it, and use it to predict labels for new data. This is often called *`scoring`* or *`inferencing`*.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a tibble for the new real estate samples\n",
                "new_data <- tibble(\n",
                "  transaction_date = c(2013.167, 2013.000),\n",
                "  house_age = c(16.2, 13.6),\n",
                "  transit_distance = c(289.3248, 4082.015),\n",
                "  local_convenience_stores = c(5, 0),\n",
                "  latitude = c(24.98203, 24.94155),\n",
                "  longitude = c(121.54348, 121.50381))\n",
                "\n",
                "# Print out new data\n",
                "new_data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have our data, let's load the saved model and make predictions.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the model into the current R session\n",
                "loaded_model <- readRDS(\"rf_price_model.rds\")\n",
                "\n",
                "# Make predictions\n",
                "predictions <- loaded_model %>% \n",
                "  augment(new_data = new_data)\n",
                "\n",
                "predictions\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That's it for now. In this notebook, you:\n",
                "\n",
                "- Explored the data set to understand the relationships between the predictors and outcomes\n",
                "- Preprocessed the data using recipes to make them easier for a model to use effectively.\n",
                "- Made a random forest model specifcation.\n",
                "- Bundles a recipe and model specification into a workflow.\n",
                "- Trained a model.\n",
                "- Made predictions on test set and evaluated the model performance.\n",
                "- Saved the model, loaded it and then used it to predict labels for new data.\n",
                "\n",
                "Fantastic job for coming this far! Feeling adventurous? Then, be sure to try out other regression models and tune some hyperparameters while at it.\n",
                "\n",
                "Happy Learning,\n",
                "\n",
                "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "language": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
