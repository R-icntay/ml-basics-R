{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup chunk to install and load required packages\n",
                "knitr::opts_chunk$set(warning = FALSE, message = FALSE)\n",
                "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
                "\n",
                "pacman::p_load('tidyverse', 'tidymodels', 'glmnet',\n",
                "               'randomForest', 'xgboost','patchwork',\n",
                "               'paletteer', 'here', 'doParallel', 'summarytools')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Regression - Experimenting with more powerful regression models**\n",
                "\n",
                "In the previous notebook, we used simple regression models to look at the relationship between features of a bike rentals dataset. In this notebook, we'll experiment with more complex models to improve our regression performance.\n",
                "\n",
                "Let's begin by loading the bicycle sharing data as a tibble and viewing the first few rows. We'll also split our data into training and test datasets.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the required packages and make them available in your current R session\n",
                "suppressPackageStartupMessages({\n",
                "  library(tidyverse)\n",
                "  library(tidymodels)\n",
                "  library(lubridate)\n",
                "  library(paletteer)\n",
                "})\n",
                "\n",
                "# Import the data into the R session\n",
                "bike_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv\", show_col_types = FALSE)\n",
                "\n",
                "# Parse dates then extract days\n",
                "bike_data <- bike_data %>%\n",
                "  mutate(dteday = mdy(dteday)) %>% \n",
                "  mutate(day = day(dteday))\n",
                "\n",
                "# Select desired features and labels\n",
                "bike_select <- bike_data %>% \n",
                "  select(c(season, mnth, holiday, weekday, workingday, weathersit,\n",
                "           temp, atemp, hum, windspeed, rentals)) %>% \n",
                "  mutate(across(1:6, factor))\n",
                "\n",
                "# Split 70% of the data for training and the rest for tesing\n",
                "set.seed(2056)\n",
                "bike_split <- bike_select %>% \n",
                "  initial_split(prop = 0.7,\n",
                "  # splitting data evenly on the holiday variable\n",
                "                strata = holiday)\n",
                "\n",
                "# Extract the data in each split\n",
                "bike_train <- training(bike_split)\n",
                "bike_test <- testing(bike_split)\n",
                "\n",
                "# Specify multiple regression metrics\n",
                "eval_metrics <- metric_set(rmse, rsq)\n",
                "\n",
                "\n",
                "cat(\"Training Set\", nrow(bike_train), \"rows\",\n",
                "    \"\\nTest Set\", nrow(bike_test), \"rows\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This results into the following two datasets:\n",
                "\n",
                "-   *bike_train*: subset of the dataset used to train the model.\n",
                "\n",
                "-   *bike_test*: subset of the dataset used to validate the model.\n",
                "\n",
                "Now we're ready to train a model by fitting a suitable regression algorithm to the training data.\n",
                "\n",
                "### Experiment with Algorithms\n",
                "\n",
                "The linear regression algorithm we used last time to train the model has some predictive capability, but there are many kinds of regression algorithm we could try, including:\n",
                "\n",
                "-   **Linear algorithms**: Not just the Linear Regression algorithm we used above (which is technically an *Ordinary Least Squares* algorithm), but other variants such as *Lasso* and *Ridge*.\n",
                "-   **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n",
                "-   **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n",
                "\n",
                "> **Note**: For a full list of parsnip model types and engines, see parsnip [model types and engines](https://www.tidymodels.org/find/parsnip/#models) and explore corresponding [model arguments](https://www.tidymodels.org/find/parsnip/#model-args) too.\n",
                "\n",
                "### Try Another Linear Algorithm\n",
                "\n",
                "Let's try training our regression model by using a `Lasso` (**Least absolute shrinkage and selection operator**) algorithm. In Tidymodels, we can do this easily by just changing the model specification then the rest is a breeze!\n",
                "\n",
                "Here we'll set up one model specification for lasso regression; we picked a value for `penalty` (sort of randomly) and we set `mixture = 1` to specifiy a lasso model (When mixture = 1, it is a pure lasso model).\n",
                "\n",
                "We'll also make a model specification in a more succinct way than we did last time!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a lasso model specification\n",
                "lasso_spec <- linear_reg(\n",
                "  engine = \"glmnet\",\n",
                "  mode = \"regression\",\n",
                "  penalty = 1,\n",
                "  mixture = 1)\n",
                "\n",
                "# Train a lasso regression model\n",
                "lasso_mod <- lasso_spec %>% \n",
                "  fit(rentals ~ ., data = bike_train)\n",
                "\n",
                "# Make predictions for test data\n",
                "results <- bike_test %>% \n",
                "  bind_cols(lasso_mod %>% predict(new_data = bike_test) %>% \n",
                "              rename(predictions = .pred))\n",
                "\n",
                "# Evaluate the model\n",
                "lasso_metrics <- eval_metrics(data = results,\n",
                "                                    truth = rentals,\n",
                "                                    estimate = predictions) \n",
                "\n",
                "\n",
                "# Plot predicted vs actual\n",
                "theme_set(theme_light())\n",
                "lasso_plt <- results %>% \n",
                "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
                "  geom_point(size = 1.6, color = 'darkorchid') +\n",
                "  # overlay regression line\n",
                "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
                "  ggtitle(\"Daily Bike Share Predictions\") +\n",
                "  xlab(\"Actual Labels\") +\n",
                "  ylab(\"Predicted Labels\") +\n",
                "  theme(plot.title = element_text(hjust = 0.5))\n",
                "\n",
                "# Return evaluations\n",
                "list(lasso_metrics, lasso_plt)\n",
                "  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ¤” Hmmmm... Not much of an improvement. We could improve the performance metrics by estimating the right regularization hyperparameter `penalty`. This can be figured out by using `resampling` and `tuning` the model which we'll discuss in just a few.\n",
                "\n",
                "### Try a Decision Tree Algorithm\n",
                "\n",
                "As an alternative to a linear model, there's a category of algorithms for machine learning that uses a `tree-based` approach in which the features in the dataset are examined in a series of evaluations, each of which results in a *branch* in a *decision tree* based on the feature value. At the end of each series of branches are leaf-nodes with the predicted label value based on the feature values.\\\n",
                "\\\n",
                "It's easiest to see how this works with an example. Let's train a Decision Tree regression model using the bike rental data. After training the model, the code below will print the model definition and a text representation of the tree it uses to predict label values.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a decision tree specification\n",
                "tree_spec <- decision_tree(\n",
                "  engine = \"rpart\",\n",
                "  mode = \"regression\")\n",
                "\n",
                "# Train a decision tree model \n",
                "tree_mod <- tree_spec %>% \n",
                "  fit(rentals ~ ., data = bike_train)\n",
                "\n",
                "# Print model\n",
                "tree_mod\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So now we have a tree-based model; but is it any good? Let's evaluate it with the test data. We'll also try out a new function: `augmet()`\n",
                "\n",
                "> `augmnet()` allows you to make and add model predictions to the given data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make and bind predictions to test data a\n",
                "results <- tree_mod %>% \n",
                "  augment(new_data = bike_test) %>% \n",
                "  rename(predictions = .pred)\n",
                "\n",
                "# Evaluate the model\n",
                "tree_metrics <- eval_metrics(data = results,\n",
                "                                  truth = rentals,\n",
                "                                  estimate = predictions)\n",
                "\n",
                "# Plot predicted vs actual\n",
                "tree_plt <- results %>% \n",
                "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
                "  geom_point(color = 'tomato') +\n",
                "  # overlay regression line\n",
                "  geom_smooth(method = 'lm', color = 'steelblue', se = F) +\n",
                "  ggtitle(\"Daily Bike Share Predictions\") +\n",
                "  xlab(\"Actual Labels\") +\n",
                "  ylab(\"Predicted Labels\") +\n",
                "  theme(plot.title = element_text(hjust = 0.5))\n",
                "\n",
                "# Return evaluations\n",
                "list(tree_metrics, tree_plt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The tree-based model doesn't seem to have much improvement over the linear model. We can also see that it's predicting constant values for a given range of predictors. So, what else could we try?\n",
                "\n",
                "### Try an Ensemble Algorithm\n",
                "\n",
                "Ensemble algorithms work by combining multiple base estimators to produce an optimal model, either by applying an *aggregate function* to a collection of base models (sometimes referred to a `bagging`) or by building a sequence of models that build on one another to improve predictive performance (referred to as `boosting`).\n",
                "\n",
                "For example, let's try a Random Forest model, which applies an averaging function to multiple Decision Tree models for a better overall model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For reproducibility\n",
                "set.seed(2056)\n",
                "\n",
                "# Build a random forest model specification\n",
                "rf_spec <- rand_forest() %>% \n",
                "  set_engine('randomForest') %>% \n",
                "  set_mode('regression')\n",
                "\n",
                "# Train a random forest model \n",
                "rf_mod <- rf_spec %>% \n",
                "  fit(rentals ~ ., data = bike_train)\n",
                "\n",
                "# Print model\n",
                "rf_mod\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So now we have a random forest model; but is it any good? Let's evaluate it with the test data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make and bind predictions to test data a\n",
                "results <- rf_mod %>% \n",
                "  augment(new_data = bike_test) %>% \n",
                "  rename(predictions = .pred)\n",
                "\n",
                "\n",
                "# Evaluate the model\n",
                "rf_metrics <- eval_metrics(data = results,\n",
                "                                  truth = rentals,\n",
                "                                  estimate = predictions)\n",
                "\n",
                "\n",
                "# Plot predicted vs actual\n",
                "rf_plt <- results %>% \n",
                "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
                "  geom_point(color = '#6CBE50FF') +\n",
                "  # overlay regression line\n",
                "  geom_smooth(method = 'lm', color = '#2B7FF9FF', se = F) +\n",
                "  ggtitle(\"Daily Bike Share Predictions\") +\n",
                "  xlab(\"Actual Labels\") +\n",
                "  ylab(\"Predicted Labels\") +\n",
                "  theme(plot.title = element_text(hjust = 0.5))\n",
                "\n",
                "# Return evaluations\n",
                "list(rf_metrics, rf_plt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ¤© Whoa! That's a step in the right direction.\n",
                "\n",
                "Let's also try a *boosting* ensemble algorithm. We'll use a `Gradient Boosting` estimator, which like a Random Forest algorithm builds multiple trees, but instead of building them all independently and taking the average result, each tree is `built` on the `outputs` of the `previous one` in an attempt to incrementally reduce the *loss* (error) in the model.\n",
                "\n",
                "In this tutorial, we'll demonstrate how to implement *Gradient Boosting Machines* using the **xgboost** engine.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For reproducibility\n",
                "set.seed(2056)\n",
                "\n",
                "# Build an xgboost model specification\n",
                "boost_spec <- boost_tree() %>% \n",
                "  set_engine('xgboost') %>% \n",
                "  set_mode('regression')\n",
                "\n",
                "# Train an xgboost model \n",
                "boost_mod <- boost_spec %>% \n",
                "  fit(rentals ~ ., data = bike_train)\n",
                "\n",
                "\n",
                "# Print model\n",
                "boost_mod\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the output, we actually see that Gradient Boosting Machines combine a series of base models, each of which is created sequentially and depends on the previous models, in an attempt to incrementally reduce the error in the model.\n",
                "\n",
                "So now we have an XGboost model; but is it any goodðŸ¤·? Again, let's evaluate it with the test data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make and bind predictions to test data a\n",
                "results <- boost_mod %>% \n",
                "  augment(new_data = bike_test) %>% \n",
                "  rename(predictions = .pred)\n",
                "\n",
                "# Evaluate the model\n",
                "boost_metrics <- eval_metrics(data = results,\n",
                "                                  truth = rentals,\n",
                "                                  estimate = predictions) \n",
                "\n",
                "# Plot predicted vs actual\n",
                "boost_plt <- results %>% \n",
                "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
                "  geom_point(color = '#4D3161FF') +\n",
                "  # overlay regression line\n",
                "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
                "  ggtitle(\"Daily Bike Share Predictions\") +\n",
                "  xlab(\"Actual Labels\") +\n",
                "  ylab(\"Predicted Labels\") +\n",
                "  theme(plot.title = element_text(hjust = 0.5))\n",
                "\n",
                "# Return evaluations\n",
                "list(boost_metrics, boost_plt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay not badðŸ‘Œ! We are definitely getting somewhere. Can we do better?\n",
                "\n",
                "### **Summary**\n",
                "\n",
                "Here we've tried a number of new regression algorithms to improve performance. In our notebook we'll look at `tuning` these algorithms to improve performance. Next, we'll take a look at `Data Preprocessing` and `model hyperparameters`.\n",
                "\n",
                "### **Further Reading**\n",
                "\n",
                "To learn more about Tidymodels, see the [Tidymodels documentation](https://www.tidymodels.org/).\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
