{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## **Exercise - Train and evaluate advanced clustering models**\n",
                "\n",
                "### Clustering: K-Means and Hierarchical\n",
                "\n",
                "In the last notebook, we learned that data can be broken into clusters and learned how to estimate the number of clusters in our data points by creating a series of clustering models with an incrementing number of clusters, and measuring the WCSS within each cluster. In this notebook, we will further explore K-Means clustering and also take a look at Hierarchical clustering.\n",
                "\n",
                "To get started, run the cell below to load our data\n",
                "\n",
                "> **Citation**: The seeds dataset used in the this exercise was originally published by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin, and can be downloaded from the UCI dataset repository (Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [<http://archive.ics.uci.edu/ml>]. Irvine, CA: University of California, School of Information and Computer Science).\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load the core tidyverse and tidymodels\r\n",
                "library(tidyverse)\r\n",
                "library(tidymodels)\r\n",
                "\r\n",
                "# Read the csv file into a tibble\r\n",
                "seeds <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/seeds.csv\")\r\n",
                "\r\n",
                "# Narrow down to desired features\r\n",
                "seeds_select <- seeds %>% \r\n",
                "  select(!groove_length) %>% \r\n",
                "  mutate(species = factor(species))\r\n",
                "\r\n",
                "# Specify a recipe for PCA and extract 2 PCA components\r\n",
                "features_2d <- recipe(~ ., data = seeds_select) %>% \r\n",
                "  update_role(species, new_role = \"ID\") %>% \r\n",
                "  step_normalize(all_predictors()) %>% \r\n",
                "  step_pca(all_predictors(), num_comp = 2, id = \"pca\") %>% \r\n",
                "  prep() %>% \r\n",
                "  bake(new_data = NULL)\r\n",
                "\r\n",
                "# Preprocess and obtain data for clustering\r\n",
                "# Drop target column and normalize data\r\n",
                "seeds_features<- recipe(~ ., data = seeds_select) %>% \r\n",
                "  step_rm(species) %>% \r\n",
                "  step_normalize(all_predictors()) %>% \r\n",
                "  prep() %>% \r\n",
                "  bake(new_data = NULL)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### K-Means Clustering\n",
                "\n",
                "The algorithm we used to create our test clusters is *K-Means*. This is a commonly used clustering algorithm that separates a dataset into *K* clusters of equal variance. The number of clusters, *K*, is user defined. The basic algorithm has the following steps:\n",
                "\n",
                "1.  A set of K centroids are randomly chosen.\n",
                "\n",
                "2.  Clusters are formed by assigning the data points to their closest centroid.\n",
                "\n",
                "3.  The means of each cluster is computed and the centroid is moved to the mean.\n",
                "\n",
                "4.  Steps 2 and 3 are repeated until a stopping criteria is met. Typically, the algorithm terminates when each new iteration results in negligable movement of centroids and the clusters become static.\n",
                "\n",
                "5.  When the clusters stop changing, the algorithm has *converged*, defining the locations of the clusters - note that the random starting point for the centroids means that re-running the algorithm could result in slightly different clusters, so training usually involves multiple iterations, reinitializing the centroids each time, and the model with the best WCSS is selected.\n",
                "\n",
                "Now, back to our previous notebok. After creating a series of clustering models with different numbers of clusters and plotting the WCSS across the clusters, we noticed a bend at around `k = 3`. This bend indicates that additional clusters beyond the third have little value and that there are two to three reasonably well separated clusters of data points.\n",
                "\n",
                "So, let's perform *K-Means* clustering specifying `k = 3` clusters and add the classifications to the data set using `augment`.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "set.seed(2056)\r\n",
                "# Fit and predict clusters with k = 3\r\n",
                "final_kmeans <- kmeans(seeds_features, centers = 3, nstart = 100, iter.max = 1000)\r\n",
                "\r\n",
                "# Add cluster prediction to the data set\r\n",
                "results_kmeans <- augment(final_kmeans, seeds_features) %>% \r\n",
                "# Bind pca_data - features_2d\r\n",
                "  bind_cols(features_2d)\r\n",
                "\r\n",
                "results_kmeans %>% \r\n",
                "  slice_head(n = 5)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's see those cluster assignments with the two dimensional data points. We'll add some touch of interactivity using the [plotly package](https://plotly.com/r/getting-started/), so feel free to hover.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "library(plotly)\r\n",
                "# Plot km_cluster assignmnet on the PC data\r\n",
                "cluster_plot <- results_kmeans %>% \r\n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\r\n",
                "  geom_point(aes(shape = .cluster), size = 2) +\r\n",
                "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\r\n",
                "\r\n",
                "# Make plot interactive\r\n",
                "ggplotly(cluster_plot)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Hopefully, the data has been separated into three distinct clusters.\n",
                "\n",
                "So what's the practical use of clustering? In some cases, you may have data that you need to group into distict clusters without knowing how many clusters there are or what they indicate. For example a marketing organization might want to separate customers into distinct segments, and then investigate how those segments exhibit different purchasing behaviors.\n",
                "\n",
                "Sometimes, clustering is used as an initial step towards creating a classification model. You start by identifying distinct groups of data points, and then assign class labels to those clusters. You can then use this labelled data to train a classification model.\n",
                "\n",
                "In the case of the seeds data, the different species of seed are already known and encoded as 0 (*Kama*), 1 (*Rosa*), or 2 (*Canadian*), so we can use these identifiers to compare the species classifications to the clusters identified by our unsupervised algorithm\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot km_cluster assignmnet on the PC data\r\n",
                "clust_spc_plot <- results_kmeans %>% \r\n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\r\n",
                "  geom_point(aes(shape = .cluster, color = species), size = 2, alpha = 0.8) +\r\n",
                "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\r\n",
                "\r\n",
                "# Make plot interactive\r\n",
                "ggplotly(clust_spc_plot)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "There may be some differences between the cluster assignments and class labels as shown by the different colors (species) within each cluster (shape). But the K-Means model should have done a reasonable job of clustering the observations so that seeds of the same species are generally in the same cluster. ðŸ’ª\n",
                "\n",
                "### **Hierarchical Clustering**\n",
                "\n",
                "Hierarchical clustering methods make fewer distributional assumptions when compared to K-means methods. However, K-means methods are generally more scalable, sometimes very much so.\n",
                "\n",
                "Hierarchical clustering creates clusters by either a *divisive* method or *agglomerative* method. The `divisive` method is a `top down` approach starting with the entire dataset and then finding partitions in a stepwise manner. `Agglomerative clustering` is a `bottom up` approach. In this lab you will work with agglomerative clustering, commonly referred to as `AGNES` (AGglomerative NESting), which roughly works as follows:\n",
                "\n",
                "1.  The linkage distances between each of the data points is computed.\n",
                "\n",
                "2.  Points are clustered pairwise with their nearest neighbor.\n",
                "\n",
                "3.  Linkage distances between the clusters are computed.\n",
                "\n",
                "4.  Clusters are combined pairwise into larger clusters.\n",
                "\n",
                "5.  Steps 3 and 4 are repeated until all data points are in a single cluster.\n",
                "\n",
                "A fundamental question in hierarchical clustering is: *How do we measure the dissimilarity between two clusters of observations?* The linkage function/agglomeration methods can be computed in a number of ways:\n",
                "\n",
                "-   Ward's minimum variance method: Minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance are merged. Tends to produce more compact clusters.\n",
                "\n",
                "-   Average linkage uses the mean pairwise distance between the members of the two clusters. Can vary in the compactness of the clusters it creates.\n",
                "\n",
                "-   Complete or Maximal linkage uses the maximum distance between the members of the two clusters. Tends to produce more compact clusters.\n",
                "\n",
                "Several different distance metrics are used to compute linkage functions:\n",
                "\n",
                "-   Euclidian or l2 distance is the most widely used. This is the only metric for the Ward linkage method.\n",
                "\n",
                "-   Manhattan or l1 distance is robust to outliers and has other interesting properties.\n",
                "\n",
                "-   Cosine similarity, is the dot product between the location vectors divided by the magnitudes of the vectors. Notice that this metric is a measure of similarity, whereas the other two metrics are measures of difference. Similarity can be quite useful when working with data such as images or text documents.\n",
                "\n",
                "Hierarchical clustering results can be easily visualized using an attractive tree-based representation called a *dendrogram*. Once the dendrogram has been constructed, we slice this structure horizontally to identify the clusters formed.\n",
                "\n",
                "### **Agglomerative Clustering**\n",
                "\n",
                "Let's see an example of clustering the seeds data using an agglomerative clustering algorithm.\n",
                "\n",
                "Let's see an example of clustering the seeds data using an agglomerative clustering algorithm. There are many functions available in R for hierarchical clustering.\n",
                "\n",
                "The [`hclust()`](https://rdrr.io/r/stats/hclust.html) function is one way to perform hierarchical clustering in R. It only needs one input and that is a distance matrix structure computed using distance metrics (e.g euclidean) as produced by [`dist()`](https://rdrr.io/pkg/factoextra/man/dist.html). `hclust()` also allows us to specify the agglomeration method to be used (i.e. `\"complete\"`, `\"average\"`, `\"single\"`, or `\"ward.D\"`).\n",
                "\n",
                "Great! Let's fit multiple hierarchical clustering models based on different agglomeration methods and see how the choice in agglomeration method changes the clustering.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# For reproducibility\r\n",
                "set.seed(2056)\r\n",
                "\r\n",
                "# Distance between observations matrix\r\n",
                "d <- dist(x = seeds_features, method = \"euclidean\")\r\n",
                "\r\n",
                "# Hierarchical clustering using Complete Linkage\r\n",
                "seeds_hclust_complete <- hclust(d, method = \"complete\")\r\n",
                "\r\n",
                "# Hierarchical clustering using Average Linkage\r\n",
                "seeds_hclust_average <- hclust(d, method = \"average\")\r\n",
                "\r\n",
                "# Hierarchical clustering using Ward Linkage\r\n",
                "seeds_hclust_ward <- hclust(d, method = \"ward.D2\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The [factoextra package](https://rpkgs.datanovia.com/factoextra/index.html) provides functions ([`fviz_dend()`](https://rdrr.io/pkg/factoextra/man/fviz_dend.html)) to visualize hierarchical clustering. Let's visualize the dendrogram representation of the clusters starting with the *Complete aggromeration* method.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "library(factoextra)\r\n",
                "\r\n",
                "# Visualize cluster separations\r\n",
                "fviz_dend(seeds_hclust_complete, main = \"Complete Linkage\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "What about Average linkage?\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Visualize cluster separations\r\n",
                "fviz_dend(seeds_hclust_average, main = \"Average Linkage\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Lastly, the ward linkage.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Visualize cluster separations\r\n",
                "fviz_dend(seeds_hclust_ward, main = \"Ward Linkage\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Perfect! Take a moment and analyze the nature of the clusters.\n",
                "\n",
                "We can do this mathematically by evaluating the *aggromerative coefficient (AC)*, which measures the clustering structure of the dataset- with values closer to 1 suggest a more balanced clustering structure and values closer to 0 suggest less well-formed clusters. `cluster::agnes()` allows us to compute the hierarchical clustering as well as this metric too.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "library(cluster)\r\n",
                "#Compute ac values\r\n",
                "ac_metric <- list(\r\n",
                "  complete_ac = agnes(seeds_features, metric = \"euclidean\", method = \"complete\")$ac,\r\n",
                "  average_ac = agnes(seeds_features, metric = \"euclidean\", method = \"average\")$ac,\r\n",
                "  ward_ac = agnes(seeds_features, metric = \"euclidean\", method = \"ward\")$ac\r\n",
                ")\r\n",
                "\r\n",
                "ac_metric\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "As we explained earlier, complete and ward linkages tend to produce tight clustering of objects.\n",
                "\n",
                "Now, let's determine the optimal number of clusters. Although hierarchical clustering does not require one to pre-specify the number of clusters, one still needs to specify the number of clusters to extract. Let's use the *WCSS* method to determine the optimal number of clusters.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Determine and visuzalize optimal n.o of clusters\r\n",
                "#  hcut (for hierarchical clustering)\r\n",
                "fviz_nbclust(seeds_features, FUNcluster = hcut, method = \"wss\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Just like in K-Means clustering, the optimal number of clusters for this data set is 3.\n",
                "\n",
                "Let's color our dendrogram according to k = 3 and observe how observations will be grouped. We'll go with the *ward* linkage method.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Visualize clustering structure for 3 groups\r\n",
                "fviz_dend(seeds_hclust_ward, k = 3, main = \"Ward Linkage\")\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Plausible enough!\n",
                "\n",
                "We can now go ahead and `cut` the hierarchical clustering model into three clusters and extract the cluster labels for each observation associated with a given cut. This is done using `cutree()`\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Hierarchical clustering using Ward Linkage\r\n",
                "seeds_hclust_ward <- hclust(d, method = \"ward.D2\")\r\n",
                "\r\n",
                "# Group data into 3 clusters\r\n",
                "results_hclust <- tibble(\r\n",
                "  cluster_id = cutree(seeds_hclust_ward, k = 3)) %>% \r\n",
                "  mutate(cluster_id = factor(cluster_id)) %>% \r\n",
                "  bind_cols(features_2d)\r\n",
                "\r\n",
                "results_hclust %>% \r\n",
                "  slice_head(n = 5)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We could probably do a little comparison between *K-Means* and *Hierarchical clustering* by counting the number of observations of each species in the corresponding clusters.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Compare k-m and hc\r\n",
                "results_hclust %>% \r\n",
                "  count(species, cluster_id) %>% \r\n",
                "  rename(cluster_id_hclust = cluster_id, n_hclust = n) %>% \r\n",
                "  bind_cols(results_kmeans %>% \r\n",
                "              count(species, .cluster) %>%\r\n",
                "              select(!species) %>% \r\n",
                "              rename(cluster_id_kmclust = .cluster, n_kmclust = n))\r\n",
                "            \r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Ignoring the cluster_id columns since they are arbitrary, we can see that the observations were grouped quite similarly by the two algorithms. We could of course make a confusion matrix to better visualize this, but we'll leave it at that for now.\n",
                "\n",
                "Let's wrap it up by making some plots showing how our observations were grouped into clusters.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot h-cluster assignmnet on the PC data\r\n",
                "hclust_spc_plot <- results_hclust %>% \r\n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\r\n",
                "  geom_point(aes(shape = cluster_id, color = species), size = 2, alpha = 0.8) +\r\n",
                "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\r\n",
                "\r\n",
                "# Make plot interactive\r\n",
                "ggplotly(hclust_spc_plot)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Summary**\n",
                "\n",
                "Here we practiced using K-means and hierarchical clustering. This unsupervised learning has the ability to take unlabelled data and identify which of these data are similar to one another.\n"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}